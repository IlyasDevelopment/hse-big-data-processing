# Форматы файлов для обработки больших данных в ETL

> **Семинар 2 -- Дополнительный теоретический материал**
> Продолжительность: ~20 минут лекции
> Предварительные требования: знакомство с форматом Parquet (рассматривается в отдельной 30-минутной презентации)

---

## Содержание

1. [Введение](#1-введение)
2. [Apache Avro](#2-apache-avro)
3. [Apache ORC (Optimized Row Columnar)](#3-apache-orc-optimized-row-columnar)
4. [CSV (Comma-Separated Values)](#4-csv-comma-separated-values)
5. [JSON / JSONL (JSON Lines)](#5-json--jsonl-json-lines)
6. [Apache Arrow (колоночный формат в оперативной памяти)](#6-apache-arrow-колоночный-формат-в-оперативной-памяти)
7. [Обзор алгоритмов сжатия](#7-обзор-алгоритмов-сжатия)
8. [Комплексное сравнение форматов](#8-комплексное-сравнение-форматов)
9. [Основные выводы](#9-основные-выводы)
10. [Литература и дополнительные материалы](#10-литература-и-дополнительные-материалы)

---

## 1. Введение

При построении ETL-конвейеров (Extract, Transform, Load) в системах обработки больших данных выбор формата файлов оказывает существенное влияние на производительность, стоимость хранения и совместимость компонентов системы. Не существует единого "лучшего" формата -- правильный выбор зависит от характеристик нагрузки: является ли система преимущественно читающей или пишущей, структурированы ли данные или полуструктурированы, требуется ли эволюция схемы, и какой движок запросов используется на следующем этапе.

Данный материал охватывает наиболее широко используемые форматы файлов в экосистеме больших данных **помимо Apache Parquet**, который рассматривается в отдельной 30-минутной презентации. Мы разберём внутреннюю архитектуру каждого формата, его сильные и слабые стороны, а также оптимальные сценарии использования. В конце мы сведём всё воедино в комплексной сравнительной таблице, включающей Parquet для полноты картины.

### Фундаментальный выбор модели хранения

Прежде чем перейти к отдельным форматам, стоит вспомнить два фундаментальных подхода к организации данных на диске:

- **Строчное хранение** записывает все поля одной записи последовательно. Это оптимально для нагрузок, при которых считываются или записываются целые строки -- транзакционные системы, потоковая загрузка данных и поиск на уровне записей.

- **Колоночное хранение** записывает все значения одного столбца последовательно. Это оптимально для аналитических запросов, которые обращаются лишь к подмножеству столбцов, поскольку движок может полностью пропустить ненужные столбцы и выиграть от значительно лучших коэффициентов сжатия однородных данных.

Большинство современных форматов для больших данных чётко относятся к одному из двух лагерей, хотя некоторые (например, ORC и Parquet) используют гибридный подход с группами строк, содержащими колоночные данные.

---

## 2. Apache Avro

### 2.1 История и происхождение

Apache Avro был создан в 2009 году **Дугом Каттингом (Doug Cutting)** -- тем же инженером, который создал Apache Hadoop и Apache Lucene. Каттинг разработал Avro специально для устранения недостатков существующего фреймворка сериализации Hadoop (Writables), который был тесно связан с Java и делал межъязыковой обмен данными болезненным.

Avro стал проектом верхнего уровня Apache в 2010 году и с тех пор стал стандартом де-факто для **сериализации данных** в потоковых архитектурах, особенно в экосистеме Apache Kafka. Его философия проектирования приоритизирует эволюцию схемы, компактное бинарное кодирование и языковую нейтральность.

### 2.2 Система схем

Одной из наиболее отличительных особенностей Avro является его **богатый язык определения схем на основе JSON**. Каждый файл Avro содержит свою схему непосредственно в заголовке файла, что означает, что данные всегда являются самоописывающими -- читателю никогда не нужен внешний реестр схем для интерпретации байтов (хотя реестры схем, такие как Confluent Schema Registry, широко используются в развёртываниях Kafka для повышения эффективности).

#### Примитивные типы

Avro поддерживает следующие примитивные типы:

| Тип       | Описание                                |
|-----------|-----------------------------------------|
| `null`    | Отсутствие значения                     |
| `boolean` | Двоичное true/false                     |
| `int`     | 32-битное целое число со знаком         |
| `long`    | 64-битное целое число со знаком         |
| `float`   | 32-битное число с плавающей точкой IEEE 754 |
| `double`  | 64-битное число с плавающей точкой IEEE 754 |
| `bytes`   | Последовательность 8-битных беззнаковых байтов |
| `string`  | Последовательность символов Unicode (UTF-8) |

#### Сложные типы

Avro также поддерживает сложные типы: `record`, `enum`, `array`, `map`, `union` и `fixed`. Они могут быть произвольно вложены друг в друга.

#### Пример определения схемы

Ниже приведён пример схемы Avro, определяющей запись пользовательского события:

```json
{
  "type": "record",
  "name": "UserEvent",
  "namespace": "com.example.analytics",
  "doc": "Represents a single user interaction event",
  "fields": [
    {
      "name": "event_id",
      "type": "string",
      "doc": "Unique identifier for the event"
    },
    {
      "name": "user_id",
      "type": "long"
    },
    {
      "name": "event_type",
      "type": {
        "type": "enum",
        "name": "EventType",
        "symbols": ["CLICK", "VIEW", "PURCHASE", "LOGOUT"]
      }
    },
    {
      "name": "timestamp",
      "type": "long",
      "logicalType": "timestamp-millis"
    },
    {
      "name": "metadata",
      "type": ["null", {
        "type": "map",
        "values": "string"
      }],
      "default": null,
      "doc": "Optional key-value metadata"
    },
    {
      "name": "tags",
      "type": {
        "type": "array",
        "items": "string"
      },
      "default": []
    }
  ]
}
```

Несколько замечаний по данному примеру:

- Поле `"namespace"` обеспечивает логическую группировку (аналогично пакету в Java).
- Поля `"doc"` служат встроенной документацией.
- Поле `metadata` использует **объединённый тип (union)** (`["null", {"type": "map", ...}]`) для выражения того, что поле является необязательным. Значение `default: null` означает, что его можно не указывать при записи.
- Аннотация `"logicalType"` для поля `timestamp` указывает потребителям, как интерпретировать базовое значение типа `long` -- в данном случае как миллисекунды с начала эпохи Unix.

#### Эволюция схемы

Эволюция схемы -- одно из сильнейших преимуществ Avro. Avro поддерживает следующие правила эволюции:

| Изменение | Поддерживается? | Примечания |
|-----------|-----------------|------------|
| Добавление поля со значением по умолчанию | Да | Старые читатели игнорируют новое поле; новые читатели используют значение по умолчанию при чтении старых данных |
| Удаление поля, имевшего значение по умолчанию | Да | Старые читатели используют значение по умолчанию; новые читатели игнорируют удалённое поле |
| Переименование поля | Да | С использованием `"aliases"` в схеме |
| Изменение типа поля (продвижение) | Частично | Допускаются только определённые продвижения: `int` -> `long` -> `float` -> `double` |
| Добавление символа перечисления | Да | Но только в конец списка символов для обратной совместимости |

Ключ к безопасной эволюции схемы -- понятие **режимов совместимости**:

- **Обратная совместимость (backward compatible)**: новая схема может читать данные, записанные со старой схемой.
- **Прямая совместимость (forward compatible)**: старая схема может читать данные, записанные с новой схемой.
- **Полная совместимость (full compatible)**: оба направления работают.

В архитектурах на основе Kafka **Confluent Schema Registry** автоматически обеспечивает соблюдение этих правил совместимости, отклоняя изменения схемы, которые нарушили бы работу потребителей.

### 2.3 Строчная модель хранения

Avro -- это **строчно-ориентированный** формат. Каждая запись сериализуется как непрерывная последовательность байтов, где поля записываются в порядке их появления в схеме. Это делает Avro отличным выбором для:

- **Нагрузок с интенсивной записью**: добавление новой записи -- это простая последовательная запись.
- **Обработки на уровне записей**: чтение целой записи -- это одно последовательное сканирование.
- **Потоковой обработки**: каждая запись может быть декодирована независимо (при наличии схемы), что идеально подходит для пообъектной обработки в Kafka.

Однако строчная ориентация означает, что Avro -- **не** лучший выбор для аналитических запросов, которым нужны лишь несколько столбцов из многих. В таких случаях движок должен десериализовать целые записи, даже если ему нужно одно-два поля, расходуя пропускную способность ввода-вывода и процессорные циклы впустую.

### 2.4 Бинарное кодирование и сериализация

Avro использует компактное **бинарное кодирование**, которое исключает имена полей и теги типов из сериализованных данных. Поскольку схема известна и при записи, и при чтении, кодировщик просто записывает значения в порядке полей схемы, а декодировщик считывает их обратно в том же порядке. Это обеспечивает очень компактное представление:

- **Целые числа** кодируются с использованием переменной длины и зигзагообразного кодирования (малые значения занимают меньше байтов).
- **Строки** имеют префикс длины.
- **Объединённые типы (union)** предваряются индексом с нуля, указывающим, какая ветвь объединения присутствует.
- **Массивы и словари** записываются как серия блоков, где каждый блок содержит счётчик, за которым следует соответствующее количество элементов, и завершается блоком с нулевым счётчиком.

Поскольку имена полей не повторяются в данных (в отличие от JSON), Avro достигает значительно лучшей эффективности хранения по сравнению с текстовыми форматами.

### 2.5 Структура файла (файлы-контейнеры объектов)

**Файл-контейнер объектов (Object Container File)** Avro (`.avro`) имеет следующую физическую структуру:

```
+------------------------------------------------------+
|                    ЗАГОЛОВОК ФАЙЛА                    |
|  - Магические байты: 4 байта ("Obj" + 0x01)          |
|  - Метаданные файла (map строка -> байты):           |
|      "avro.schema" -> строка JSON-схемы              |
|      "avro.codec"  -> имя кодека сжатия              |
|  - 16-байтовый маркер синхронизации (случайный)      |
+------------------------------------------------------+
|                   БЛОК ДАННЫХ 1                       |
|  - Количество объектов в блоке (long)                |
|  - Размер сериализованных объектов в байтах (long)   |
|  - Сериализованные объекты (возможно, сжатые)        |
|  - 16-байтовый маркер синхронизации                  |
+------------------------------------------------------+
|                   БЛОК ДАННЫХ 2                       |
|  - Количество объектов в блоке (long)                |
|  - Размер сериализованных объектов в байтах (long)   |
|  - Сериализованные объекты (возможно, сжатые)        |
|  - 16-байтовый маркер синхронизации                  |
+------------------------------------------------------+
|                      ...                              |
+------------------------------------------------------+
```

Ключевые архитектурные особенности:

- **Маркер синхронизации** -- это 16-байтовое случайное значение, генерируемое один раз для файла и повторяемое в конце каждого блока данных. Он выполняет две функции: (1) позволяет читателю находить границы блоков при поиске в середине файла, и (2) обеспечивает базовый механизм обнаружения повреждений.
- Каждый **блок данных** сжимается независимо (если сжатие включено), что означает возможность параллельной декомпрессии блоков.
- Файл **является разделяемым (splittable)** по границам блоков. Это критически важно для фреймворков распределённой обработки, таких как MapReduce и Spark, которым необходимо назначать разные части файла разным рабочим узлам.

### 2.6 Поддержка сжатия

Avro поддерживает несколько кодеков сжатия, применяемых на уровне блока данных:

| Кодек        | Степень сжатия | Скорость    | Примечания |
|--------------|----------------|-------------|------------|
| `null`       | Нет            | Максимальная | Без сжатия, сырые байты |
| `deflate`    | Высокая        | Медленная   | Совместим с GZIP, широко поддерживается |
| `snappy`     | Умеренная      | Быстрая     | Хороший выбор по умолчанию для большинства нагрузок |
| `bzip2`      | Очень высокая  | Очень медленная | Лучшая степень сжатия, редко используется на практике |
| `zstandard`  | Высокая        | Быстрая     | Лучший баланс; набирает популярность |
| `xz`         | Очень высокая  | Очень медленная | Экстремальное сжатие |

На практике **Snappy** и **Zstandard** -- наиболее часто используемые кодеки в конвейерах обработки больших данных. Snappy -- традиционный выбор, когда пропускная способность важнее степени сжатия; Zstandard (`zstd`) всё чаще предпочитают, поскольку он достигает степени сжатия, близкой к GZIP, при скорости, близкой к Snappy.

### 2.7 Сценарии использования

Avro -- лучший выбор, когда:

- **Эволюция схемы -- обязательное требование.** Правила совместимости Avro являются наиболее зрелыми и хорошо поддерживаемыми инструментами в экосистеме.
- **Нагрузка ориентирована на запись или добавление.** Строчно-ориентированная сериализация означает дешёвую запись.
- **Данные проходят через Apache Kafka.** Avro -- стандартный формат сериализации для сообщений Kafka с первоклассной поддержкой от Confluent Schema Registry.
- **Необходима межъязыковая совместимость.** Avro имеет официальные библиотеки для Java, Python, C, C++, C#, Ruby и других языков.
- **Данные впоследствии будут конвертированы в колоночный формат.** Распространённый паттерн: загружать данные как Avro в потоковый слой, а затем периодически компактировать их в Parquet для аналитических запросов.

Avro -- **не** лучший выбор, когда:

- Нагрузка является аналитической (выбор нескольких столбцов из широких таблиц).
- Приоритетом является максимальная производительность запросов на больших наборах данных.
- Данные потребляются преимущественно SQL-движками запросов.

### 2.8 Сравнение Avro и Parquet

| Характеристика              | Avro                          | Parquet                        |
|-----------------------------|-------------------------------|--------------------------------|
| Модель хранения             | Строчная                      | Колоночная (гибрид с группами строк) |
| Расположение схемы          | Встроена в заголовок файла    | Встроена в подвал (footer) файла |
| Эволюция схемы              | Отличная (первоклассная)      | Хорошая (но более ограниченная) |
| Скорость записи             | Быстрая (последовательное добавление) | Медленнее (колоночная реорганизация) |
| Скорость чтения (полная строка) | Быстрая                  | Умеренная (необходимо восстановить строку) |
| Скорость чтения (несколько столбцов) | Медленная (читает все столбцы) | Очень быстрая (отсечение столбцов) |
| Степень сжатия              | Умеренная                     | Высокая (однородные столбцы)    |
| Разделяемость (splittable)  | Да (по границам блоков)       | Да (по границам групп строк)    |
| Вложенные данные            | Да (union, array, map)        | Да (кодирование Dremel)         |
| Основной сценарий           | Сериализация, потоковая обработка, этап записи ETL | Аналитика, этап чтения ETL |
| Интеграция с экосистемой    | Kafka, Flink, NiFi            | Spark, Hive, Impala, Trino, DuckDB |

---

## 3. Apache ORC (Optimized Row Columnar)

### 3.1 История и происхождение

Apache ORC (Optimized Row Columnar) был создан компанией **Hortonworks** в 2013 году специально для оптимизации производительности запросов в **Apache Hive**. Исходная мотивация заключалась в том, что стандартное текстовое хранилище Hive (CSV/TSV-файлы) и даже его более ранний колоночный формат (RCFile) были слишком медленными для интерактивной аналитики. ORC был спроектирован с нуля для минимизации ввода-вывода, максимизации сжатия и поддержки предикатного pushdown -- всё в рамках экосистемы Hive/HDFS.

ORC стал проектом верхнего уровня Apache в 2015 году. Хотя он расширил поддержку экосистемы (Spark, Presto/Trino и Flink могут читать ORC), он по-прежнему наиболее тесно связан с экосистемой **Hive/HDFS**. В отличие от него, Parquet добился более широкого распространения в мультидвижковых средах.

### 3.2 Колоночное хранение со страйпами

Как и Parquet, ORC использует **гибридный подход группа строк/колоночное хранение**. Файл делится на большие горизонтальные разделы, называемые **страйпами (stripes)** (аналог групп строк в Parquet). Каждый страйп обычно содержит около **250 000 строк** по умолчанию (хотя это настраивается, а размер страйпа обычно составляет 64-256 МБ).

Внутри каждого страйпа данные организованы по столбцам. Это означает:

- Аналитический запрос, читающий только 3 столбца из 100, будет считывать данные только из этих 3 столбцов в каждом страйпе, пропуская остальные 97.
- Данные внутри столбца однородны (все значения одного типа), что обеспечивает отличную степень сжатия.
- Структура страйпов также делает файл разделяемым для распределённой обработки.

### 3.3 Структура файла

Формат файла ORC имеет чётко определённую физическую структуру. Понимание этой структуры важно для рассуждений о производительности запросов.

```
+------------------------------------------------------+
|                     СТРАЙП 1                          |
|  +--------------------------------------------------+|
|  |              ИНДЕКСНЫЕ ДАННЫЕ                     ||
|  |  - Минимальные/максимальные значения столбцов     ||
|  |  - Позиции строк для навигации                    ||
|  |  - Необязательные данные фильтра Блума            ||
|  +--------------------------------------------------+|
|  |              ДАННЫЕ СТРОК                         ||
|  |  - Поток столбца 1: кодированные + сжатые значения||
|  |  - Поток столбца 2: кодированные + сжатые значения||
|  |  - ...                                            ||
|  |  - Поток столбца N: кодированные + сжатые значения||
|  +--------------------------------------------------+|
|  |            ПОДВАЛ СТРАЙПА                         ||
|  |  - Расположение и размеры потоков                 ||
|  |  - Информация о кодировании столбцов              ||
|  +--------------------------------------------------+|
+------------------------------------------------------+
|                     СТРАЙП 2                          |
|  (та же структура, что и выше)                        |
+------------------------------------------------------+
|                      ...                              |
+------------------------------------------------------+
|                   ПОДВАЛ ФАЙЛА                        |
|  - Список страйпов (смещение, длина, число строк)    |
|  - Информация о типах (полная схема)                  |
|  - Статистика на уровне столбцов (min, max, sum,      |
|    count, has_null) для всего файла                   |
|  - Пользовательские метаданные                        |
+------------------------------------------------------+
|                   ПОСТСКРИПТ                          |
|  - Длина подвала                                      |
|  - Кодек сжатия                                       |
|  - Размер блока сжатия                                |
|  - Версия формата ORC                                 |
|  - Магическая строка "ORC"                            |
+------------------------------------------------------+
```

Процесс чтения идёт снизу вверх:

1. Читатель сначала считывает **Постскрипт** (последние байты файла), чтобы узнать кодек сжатия и расположение подвала.
2. Затем считывает **Подвал файла**, чтобы обнаружить схему, расположение страйпов и статистику на уровне файла.
3. Для каждого страйпа читатель считывает **Индексные данные**, чтобы определить, можно ли пропустить страйп на основе предикатного pushdown.
4. Только после этого он считывает соответствующие **потоки данных строк** для столбцов, запрошенных запросом.

Этот подход "снизу вверх" эффективен, потому что метаданные файла компактны и могут быть прочитаны всего за 2-3 небольших операции ввода-вывода до обращения к фактическим данным.

### 3.4 Встроенные индексы

ORC предоставляет два уровня встроенной индексации, обеспечивающих эффективный пропуск данных:

#### Индекс группы строк (индекс Min/Max)

Внутри каждого страйпа ORC делит строки на **группы строк** по 10 000 строк (по умолчанию). Для каждой группы строк ORC записывает:

- **Минимальное значение** столбца в данной группе строк
- **Максимальное значение** столбца в данной группе строк
- **Сумму** (для числовых столбцов)
- **Количество** ненулевых значений
- **Наличие нулевых значений**

Когда запрос содержит предикат вида `WHERE age > 30`, читатель может проверить статистику min/max для столбца `age` в каждой группе строк. Если максимальное значение в группе строк равно 25, всю группу строк можно пропустить без чтения данных. Это тот же принцип, что и статистика фрагментов столбцов в Parquet, но ORC обеспечивает более мелкую гранулярность с его 10 000 строк по умолчанию.

#### Индекс фильтра Блума

Для столбцов с высокой кардинальностью (большим количеством уникальных значений) индексы min/max не очень селективны. Если столбец содержит значения от 1 до 1 000 000, практически ни одна группа строк не будет пропущена при точечном поиске вида `WHERE user_id = 42`.

ORC решает эту проблему с помощью необязательных **индексов на основе фильтра Блума**. Фильтр Блума -- это вероятностная структура данных, которая может ответить на вопрос "присутствует ли это значение в данном множестве?" со следующими гарантиями:

- **Нет ложноотрицательных результатов**: если фильтр Блума говорит "нет", значение точно отсутствует.
- **Возможны ложноположительные результаты**: если фильтр Блума говорит "возможно", значение может присутствовать или отсутствовать.

Фильтры Блума создаются для каждого столбца в каждой группе строк и могут быть включены для конкретных столбцов:

```sql
-- В DDL Hive включение фильтра Блума для конкретных столбцов:
CREATE TABLE events (
    event_id STRING,
    user_id  BIGINT,
    ts       TIMESTAMP,
    payload  STRING
)
STORED AS ORC
TBLPROPERTIES (
    "orc.bloom.filter.columns" = "event_id,user_id",
    "orc.bloom.filter.fpp"     = "0.01"
);
```

Параметр `fpp` (false positive probability -- вероятность ложноположительного результата) контролирует компромисс между размером индекса и точностью. Более низкий FPP означает больший фильтр Блума, но меньше ложноположительных результатов.

### 3.5 Схемы кодирования

ORC использует несколько стратегий кодирования для минимизации размера данных перед применением сжатия. Кодировщик автоматически выбирает лучшее кодирование для каждого столбца на основе характеристик данных.

| Кодирование | Применяется к | Как работает |
|-------------|---------------|-------------|
| **Кодирование длин серий (RLE)** | Целые числа, метки времени | Последовательные одинаковые значения хранятся как пары (значение, количество). ORC использует продвинутый "RLEv2", который обрабатывает как точные серии, так и дельта-кодированные последовательности. |
| **Словарное кодирование** | Строки с низкой-умеренной кардинальностью | Строит словарь уникальных значений и заменяет каждое вхождение коротким целочисленным индексом. Очень эффективно, когда столбец имеет мало уникальных значений (например, коды стран, значения статуса). |
| **Упаковка битов (Bit Packing)** | Булевы значения, малые целые числа | Упаковывает значения в минимально необходимое количество битов. Для булевых значений это означает 8 значений на байт. |
| **Дельта-кодирование** | Отсортированные или почти отсортированные целые числа | Хранит разницу между последовательными значениями, а не сами значения. Если значения увеличиваются на постоянную величину, дельта-кодирование сводится к очень компактному представлению. |
| **Прямое кодирование** | Строки с высокой кардинальностью | Когда словарное кодирование не приносит пользы (слишком много уникальных значений), значения хранятся напрямую с префиксом длины. |

Автоматический выбор кодирования -- одна из сильных сторон ORC: писатель анализирует данные в процессе записи и выбирает наиболее эффективное кодирование для каждого столбца без участия пользователя.

### 3.6 Сжатие

После кодирования ORC применяет блочный алгоритм сжатия. Доступные кодеки:

| Кодек   | Степень сжатия     | Нагрузка на CPU | Примечания |
|---------|--------------------|-----------------|------------|
| `NONE`  | 1.0x               | Нулевая         | Без сжатия |
| `ZLIB`  | Высокая (~5-8x)    | Высокая         | По умолчанию во многих инсталляциях Hive; хорошее сжатие, но медленно |
| `SNAPPY`| Умеренная (~3-4x)  | Низкая          | Быстрая декомпрессия; подходит для чувствительных к задержке запросов |
| `LZO`   | Умеренная (~3-4x)  | Низкая          | Аналогичен Snappy; требует отдельной установки библиотеки |
| `LZ4`   | Умеренная (~3-4x)  | Очень низкая    | Наиболее быстрая декомпрессия; минимальная нагрузка на CPU |
| `ZSTD`  | Высокая (~5-7x)    | Умеренная       | Лучший баланс; всё чаще рекомендуется по умолчанию |

Сжатие применяется к отдельным **потокам** внутри каждого страйпа при настраиваемом размере блока (по умолчанию 256 КБ). Это означает, что декомпрессия является гранулярной -- читатель распаковывает только те конкретные потоки, которые ему нужны.

Полезное эмпирическое правило: если узким местом является дисковый ввод-вывод (например, вращающиеся HDD, сетевое хранилище), предпочтительны кодеки с высокой степенью сжатия, такие как ZLIB или ZSTD. Если узким местом является CPU (быстрые SSD, много одновременных запросов), предпочтительны быстрые кодеки, такие как Snappy или LZ4.

### 3.7 Поддержка ACID-транзакций

Одно из уникальных преимуществ ORC -- поддержка **ACID-транзакций в Apache Hive**. Начиная с Hive 0.14, таблицы ORC могут поддерживать:

- Операции **INSERT, UPDATE, DELETE** (а не только добавление записей).
- **Транзакционная согласованность**: читатели видят согласованный снимок и не затрагиваются параллельными записями.
- **Компактификация**: фоновые процессы объединяют дельта-файлы в базовые файлы для поддержания производительности чтения.

Это реализовано через систему **базовых файлов** и **дельта-файлов**:

1. **Базовый файл** содержит основную массу данных в стандартном формате ORC.
2. **Дельта-файлы** записывают операции INSERT, UPDATE и DELETE как небольшие файлы ORC.
3. Процесс **компактификации** периодически объединяет дельты в базовый файл.

Это делает ORC единственным распространённым форматом файлов для больших данных, который нативно поддерживает изменяемые данные в экосистеме Hive. (Parquet достигает аналогичной функциональности через внешние табличные форматы, такие как Delta Lake, Apache Iceberg или Apache Hudi, а не на уровне формата файлов.)

### 3.8 Предикатный pushdown и проекционный pushdown

ORC поддерживает две ключевые техники оптимизации, которые радикально сокращают объём ввода-вывода:

**Предикатный pushdown** позволяет движку запросов передавать условия фильтрации (предложения WHERE) непосредственно читателю файлов. Читатель ORC использует статистику на уровне страйпов, статистику на уровне групп строк и фильтры Блума для пропуска целых фрагментов данных, которые не могут соответствовать предикату. Например:

```sql
SELECT user_id, event_type
FROM events
WHERE event_date = '2025-01-15'
  AND user_id = 42;
```

Читатель ORC:
1. Проверяет статистику на уровне страйпа для `event_date` -- пропускает страйпы, в которых диапазон min/max не включает `'2025-01-15'`.
2. Внутри подходящих страйпов проверяет статистику на уровне группы строк для дополнительного пропуска.
3. Если доступен фильтр Блума для `user_id`, проверяет, возможно ли присутствие значения `42` в каждой группе строк.

**Проекционный pushdown** (также называемый отсечением столбцов) означает, что читатель считывает только столбцы, перечисленные в предложении SELECT. В приведённом запросе с диска считываются только потоки столбцов `user_id` и `event_type` -- все остальные столбцы полностью пропускаются.

В совокупности эти оптимизации могут сократить объём данных, считываемых с диска, на 90% и более для типичных аналитических запросов.

### 3.9 ORC vs Parquet

| Характеристика            | ORC                              | Parquet                           |
|---------------------------|----------------------------------|-----------------------------------|
| Происхождение             | Hortonworks (ориентирован на Hive) | Twitter + Cloudera (кросс-движковый) |
| Основная экосистема       | Hive, HDFS                       | Spark, Trino, Impala, широкая     |
| Название группы строк     | Страйп (по умолчанию ~250K строк) | Группа строк (по умолчанию ~128 МБ) |
| Встроенные индексы        | Min/max + фильтры Блума          | Статистика min/max, индексы страниц |
| ACID-транзакции           | Нативные (в Hive)                | Через Delta Lake / Iceberg / Hudi |
| Модель вложенных данных   | Struct, list, map, union         | Уровни повторения/определения на основе Dremel |
| Сжатие по умолчанию       | ZLIB                             | Snappy (часто) или ZSTD           |
| Эволюция схемы            | Хорошая (добавление/удаление столбцов) | Хорошая (добавление/удаление столбцов) |
| Широта сообщества         | Уже (Hive-ориентирован)          | Шире (мультидвижковый)            |
| Производительность (общая) | Сопоставимая                    | Сопоставимая                      |

### 3.10 Когда выбирать ORC vs Parquet

**Выбирайте ORC, когда:**
- Ваш основной движок запросов -- **Apache Hive**.
- Вам нужна **поддержка ACID-транзакций** нативно на уровне формата файлов.
- Вы активно используете экосистему **Hortonworks / HDP**.
- Вам нужны **детализированные индексы** (фильтры Блума с минимальной настройкой).

**Выбирайте Parquet, когда:**
- Вы используете **несколько движков запросов** (Spark, Trino, DuckDB, Athena, BigQuery и т.д.).
- Вам нужна **максимально широкая совместимость с экосистемой**.
- Вы работаете с табличными форматами **Delta Lake, Iceberg или Hudi**.
- Вы строите **облачно-нативные** архитектуры (AWS, GCP, Azure -- все имеют первоклассную поддержку Parquet).

На практике разница в производительности между ORC и Parquet для большинства нагрузок невелика. Решение обычно определяется соображениями экосистемы и инструментария.

---

## 4. CSV (Comma-Separated Values)

### 4.1 Обзор

CSV -- старейший и простейший табличный формат данных, до сих пор широко используемый. CSV-файл -- это обычный текстовый файл, в котором каждая строка представляет запись, а поля внутри записи разделены символом-разделителем (традиционно запятой, но также используются табуляция, точка с запятой и вертикальная черта).

```csv
event_id,user_id,event_type,timestamp,amount
evt_001,12345,PURCHASE,2025-01-15T10:30:00Z,49.99
evt_002,12346,CLICK,2025-01-15T10:31:00Z,
evt_003,12345,VIEW,2025-01-15T10:32:00Z,
```

У CSV нет формальной спецификации, которой следуют повсеместно, хотя RFC 4180 предоставляет стандарт де-факто. На практике CSV-файлы сильно различаются в использовании кавычек, экранирования, символов перевода строки, кодировки символов и соглашений о заголовках.

### 4.2 Характеристики

| Свойство | CSV |
|----------|-----|
| Схема | Отсутствует (нет информации о типах; все значения -- строки) |
| Сжатие | Нет встроенного; возможно внешнее сжатие (gzip, bz2) |
| Человекочитаемость | Да |
| Разделяемость | Да (без сжатия) / Нет (при gzip-сжатии) |
| Вложенные данные | Не поддерживаются |
| Типобезопасность | Отсутствует |
| Эффективность кодирования | Очень низкая (текстовое представление чисел, повторяющиеся разделители) |

### 4.3 Проблемы CSV в больших данных

Хотя CSV повсеместно распространён и удобен, он имеет ряд серьёзных проблем при использовании в масштабе:

**1. Отсутствие информации о типах.** Каждое значение -- строка. Нижестоящая система должна самостоятельно определить или получить указание, что `"49.99"` -- это число с плавающей точкой, а `"2025-01-15T10:30:00Z"` -- метка времени. Такое определение типов подвержено ошибкам и требует значительных процессорных ресурсов.

**2. Отсутствие контроля схемы.** Ничто не мешает строке содержать неправильное количество полей или полю содержать значение неправильного типа. Повреждённые или некорректно сформированные строки обнаруживаются только при чтении.

**3. Конфликты разделителей.** Если значение поля содержит символ-разделитель (например, запятую в описании товара), оно должно быть заключено в кавычки. Но правила кавычек не являются универсально согласованными, что приводит к ошибкам разбора.

```csv
product_id,description,price
P001,"Widget, Deluxe Model",29.99
P002,"He said ""hello""",19.99
```

**4. Крайне низкая эффективность хранения.** Рассмотрим хранение целого числа `1000000`. В бинарном формате вроде Avro это занимает 4 байта. В CSV -- 7 байтов (символы `1`, `0`, `0`, `0`, `0`, `0`, `0`). Для чисел с плавающей точкой накладные расходы ещё больше.

**5. Отсутствие сжатия.** CSV не имеет встроенного сжатия. Файлы можно сжать gzip извне, но сжатый gzip CSV-файл **не является разделяемым** -- фреймворк распределённой обработки не может назначить разные части файла разным рабочим узлам без полной декомпрессии файла. (Форматы с блочным сжатием, такие как bzip2, разделяемы, но очень медленны.)

**6. Отсутствие вложенных данных.** CSV строго плоский/табличный. Представление иерархических или вложенных данных требует денормализации или неудобных обходных решений вроде JSON-внутри-ячейки.

### 4.4 Когда CSV всё ещё используется

Несмотря на ограничения, CSV остаётся актуальным в ряде сценариев:

- **Обмен данными с нетехническими заинтересованными сторонами.** CSV-файлы можно открыть в Excel, Google Sheets или любом текстовом редакторе.
- **Интеграция с унаследованными системами.** Многие старые системы экспортируют данные только в формате CSV.
- **Простой разовый экспорт.** Когда нужно быстро выгрузить результат запроса для ручной проверки.
- **Первоначальная загрузка данных.** Исходные данные от поставщиков, партнёров или государственных органов часто предоставляются в формате CSV. Первый шаг ETL-конвейера обычно заключается в конвертации в более эффективный формат.
- **Отладка.** Для чтения нескольких строк CSV-файла не требуется специальный инструментарий.

В хорошо спроектированном конвейере обработки больших данных CSV обычно встречается только на **границах** -- как входной формат из внешних источников или как выходной формат для восприятия человеком. Данные внутри конвейера должны храниться в бинарном формате, таком как Parquet, ORC или Avro.

### 4.5 Работа с CSV в Spark

Даже когда использование CSV неизбежно, Spark предоставляет возможности для смягчения некоторых его недостатков:

```python
df = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .option("multiLine", "true") \
    .option("escape", '"') \
    .option("dateFormat", "yyyy-MM-dd") \
    .option("timestampFormat", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'") \
    .option("mode", "PERMISSIVE") \
    .option("columnNameOfCorruptRecord", "_corrupt_record") \
    .csv("s3://bucket/raw/data.csv")

# Немедленно сохраняем в более подходящем формате для дальнейшей обработки:
df.write \
    .mode("overwrite") \
    .parquet("s3://bucket/processed/data.parquet")
```

Обратите внимание на опцию `inferSchema` -- Spark делает два прохода по данным (один для определения типов, один для разбора), что удваивает время чтения. В продакшене лучше определять схему явно.

---

## 5. JSON / JSONL (JSON Lines)

### 5.1 Обзор

JSON (JavaScript Object Notation) -- текстовый, самоописывающий формат данных, ставший лингва-франка веб-API и конфигурационных файлов. В контексте больших данных JSON встречается в двух основных формах:

- **Стандартный JSON**: один JSON-массив или объект на файл. Обычно используется для конфигураций, ответов API или небольших наборов данных.
- **JSONL (JSON Lines)**: один полный JSON-объект на строку, разделённый переводами строк. Это предпочтительная форма для больших данных, поскольку она **разделяема по строкам** -- каждая строка может быть разобрана независимо.

```jsonl
{"event_id": "evt_001", "user_id": 12345, "event_type": "PURCHASE", "amount": 49.99}
{"event_id": "evt_002", "user_id": 12346, "event_type": "CLICK", "amount": null}
{"event_id": "evt_003", "user_id": 12345, "event_type": "VIEW", "tags": ["organic", "mobile"]}
```

### 5.2 Характеристики

| Свойство | JSON/JSONL |
|----------|-----------|
| Схема | Отсутствует (схема при чтении; самоописывающий) |
| Сжатие | Нет встроенного; возможно внешнее сжатие |
| Человекочитаемость | Да |
| Разделяемость | JSONL: Да / Стандартный JSON: Нет |
| Вложенные данные | Отлично (нативная поддержка) |
| Типобезопасность | Частичная (числа, строки, булевы значения, null; но нет различия между целыми и дробными, нет типов даты/метки времени) |
| Эффективность кодирования | Очень низкая (имена полей повторяются в каждой записи, текстовое кодирование значений) |

### 5.3 Проблема многословности

Самая большая слабость JSON для больших данных -- его крайняя многословность. Рассмотрим таблицу с 10 столбцами и 1 миллионом строк:

- В **Parquet** имена столбцов хранятся один раз в метаданных. Данные кодируются в бинарном формате и сжимаются. Общий размер может составить 50 МБ.
- В **JSON** каждая отдельная запись повторяет все 10 имён полей. Данные кодируются текстом. Общий размер может составить 500 МБ и более -- **10-кратный** перерасход.

Эта многословность наносит ущерб тремя способами:

1. **Стоимость хранения**: больше байтов на диске или в объектном хранилище означает более высокую стоимость.
2. **Пропускная способность ввода-вывода**: больше байтов для чтения с диска или из сети.
3. **Время разбора**: разбор JSON требует значительных процессорных ресурсов, поскольку парсер должен обрабатывать произвольную вложенность, экранирование Unicode и разбор чисел для каждой записи.

### 5.4 Схема при чтении (schema-on-read)

JSON -- формат со **схемой при чтении**, что означает отсутствие предопределённой схемы -- структура определяется при считывании данных. Это обеспечивает максимальную гибкость, но создаёт ряд проблем:

- **Несогласованность схем**: разные записи в одном наборе данных могут иметь разные поля или разные типы для одного и того же поля. Одна запись может содержать `"age": 25` (целое число), а другая -- `"age": "twenty-five"` (строка).
- **Стоимость определения схемы**: инструменты вроде Spark должны сканировать выборку данных (или весь набор) для определения схемы, что медленно и потенциально неточно.
- **Отсутствие гарантий эволюции**: нет механизма для обеспечения обратной или прямой совместимости при изменении структуры данных.

### 5.5 Когда JSON/JSONL всё ещё используется

- **Ответы API**: большинство веб-API возвращают JSON, поэтому конвейеры загрузки часто начинают с данных JSON.
- **Логирование**: журналы приложений часто выводятся как структурированный JSON (например, из фреймворков `structlog` или `log4j2` с JSON-форматированием).
- **Конфигурационные файлы**: конфигурации Spark, DAG-файлы Airflow и другие инструменты используют JSON или YAML.
- **Исследование полуструктурированных данных**: когда схема неизвестна или быстро меняется, JSON обеспечивает максимальную гибкость для первоначального исследования.
- **Небольшие наборы данных**: для наборов данных до нескольких сотен мегабайт накладные расходы JSON часто приемлемы, а человекочитаемость ценна.

### 5.6 Оптимизация загрузки JSON

В продакшен ETL-конвейерах JSON следует рассматривать как **формат зоны приземления** и конвертировать в колоночный формат как можно раньше:

```python
# Чтение JSON Lines из потокового источника
raw_df = spark.read \
    .option("multiLine", "false") \
    .schema(explicit_schema) \
    .json("s3://bucket/raw/events/*.jsonl")

# Валидация, трансформация и сохранение как Parquet
raw_df \
    .filter(col("event_id").isNotNull()) \
    .withColumn("event_date", to_date("timestamp")) \
    .write \
    .partitionBy("event_date") \
    .parquet("s3://bucket/curated/events/")
```

Ключевые рекомендации:

- **Всегда указывайте явную схему**, а не полагайтесь на `inferSchema`. Это позволяет избежать дорогостоящего прохода определения типов и обеспечивает согласованность типов.
- **Используйте JSONL** (одна запись на строку) вместо стандартного JSON для больших наборов данных. JSONL разделяем и поддерживает потоковую обработку.
- **Конвертируйте в колоночный формат** (Parquet или ORC) как можно раньше в конвейере.

---

## 6. Apache Arrow (колоночный формат в оперативной памяти)

### 6.1 Что такое Arrow (и чем он не является)

Apache Arrow -- это **не файловый формат** в традиционном смысле. Это **языконезависимая спецификация колоночного представления данных в оперативной памяти**. Представьте его как стандартизированный способ для различных инструментов и языков обмениваться колоночными данными в памяти без затрат на сериализацию и десериализацию.

Arrow был создан в 2016 году Уэсом МакКинни (Wes McKinney, создатель pandas) и несколькими другими лидерами сообщества обработки данных. Мотивация была проста: каждая библиотека обработки данных (pandas, R data frames, Spark, Drill, Impala и т.д.) имела свой собственный внутренний формат в памяти. Передача данных между любыми двумя инструментами требовала сериализации (преобразования из формата A в байты) и десериализации (преобразования из байтов в формат B), что было медленным и расточительным.

Arrow устраняет это, определяя единый стандартизированный формат в памяти, с которым могут работать все инструменты.

### 6.2 Ключевые принципы проектирования

**Чтение без копирования (zero-copy reads)**: когда данные уже находятся в формате Arrow в памяти, другой процесс или библиотека может читать их напрямую, без копирования или конвертации. Это является революционным для межпроцессного взаимодействия (IPC) -- например, Python-процесс может обмениваться данными с Java-процессом без каких-либо затрат на сериализацию.

**Языковая независимость**: Arrow имеет реализации на C++, Java, Python (PyArrow), Rust, Go, JavaScript, Julia, R и других языках. Все реализации используют одинаковый формат в памяти, поэтому данные можно передавать через языковые границы.

**Кэш-дружественная колоночная раскладка**: Arrow организует данные в кэш-эффективном колоночном формате с значениями фиксированной ширины, хранящимися в непрерывных массивах, и значениями переменной ширины (строками), хранящимися в парах буферов смещение/значение. Эта раскладка оптимизирована для SIMD-векторизованной обработки на современных процессорах.

**Обработка null через битовые маски валидности**: вместо использования значений-маркеров или необязательных обёрток Arrow использует компактную **битовую маску валидности**, где каждый бит указывает, является ли соответствующее значение null. Это одновременно компактно и дружественно к предсказанию переходов.

### 6.3 Раскладка в памяти

Массив Arrow для столбца `int32` со значениями `[1, null, 3, 4, null]` выглядит в памяти следующим образом:

```
Битовая маска валидности:  [1, 0, 1, 1, 0]   (упакованные биты: 0b00011010 = 0x1A)
                            ^  ^  ^  ^  ^
                            |  |  |  |  +-- индекс 4: null
                            |  |  |  +---- индекс 3: валидно
                            |  |  +------- индекс 2: валидно
                            |  +---------- индекс 1: null
                            +------------- индекс 0: валидно

Буфер значений:            [1, ?, 3, 4, ?]   (4 байта каждое, всего 20 байтов)
                                ^        ^
                                |        +-- неопределено (null-слот, может быть любым)
                                +----------- неопределено (null-слот)
```

Для типов переменной длины, таких как строки, Arrow использует буфер смещений:

```
Значения столбца: ["hello", "big", null, "data"]

Буфер смещений:  [0, 5, 8, 8, 12]   (int32, 5 записей для 4 значений)
Буфер данных:    h e l l o b i g d a t a
                 |         |     |   |
                 0         5     8   12

Битовая маска валидности: [1, 1, 0, 1]
```

Третье значение -- null, поэтому его диапазон смещений (от 8 до 8) имеет нулевую длину, а битовая маска валидности отмечает его как невалидное.

### 6.4 Формат IPC и Feather (v2)

Хотя Arrow -- прежде всего спецификация для данных в оперативной памяти, он также определяет **формат IPC (Inter-Process Communication)** для сериализации данных Arrow в байты (для записи в файлы, отправки по сети или обмена между процессами через разделяемую память).

Формат Arrow IPC существует в двух вариантах:

- **Потоковый формат**: последовательность пакетов записей, подходящая для потоковой передачи или конвейерной обработки между процессами. Произвольный доступ отсутствует -- читатель должен обрабатывать пакеты последовательно.
- **Файловый формат (также известный как Feather v2 или Arrow IPC file)**: добавляет подвал файла с метаданными, обеспечивающими произвольный доступ к отдельным пакетам записей. Файлы обычно используют расширение `.arrow` или `.feather`.

Формат Feather изначально был создан Уэсом МакКинни и Хэдли Уикхэмом (Hadley Wickham, из мира R) как быстрый, языконезависимый файловый формат для таблиц данных. Feather v2 -- это просто файловый формат Arrow IPC, и он заменил оригинальный Feather v1.

```python
import pyarrow as pa
import pyarrow.feather as feather

# Создание таблицы Arrow
table = pa.table({
    "user_id": [1, 2, 3, 4, 5],
    "name": ["Alice", "Bob", "Charlie", "Diana", "Eve"],
    "score": [95.5, 87.3, None, 91.0, 78.2]
})

# Запись в Feather v2 (файл Arrow IPC)
feather.write_feather(table, "users.feather", compression="zstd")

# Чтение обратно -- чрезвычайно быстро, почти zero-copy
table2 = feather.read_feather("users.feather")
```

Файлы Feather/Arrow IPC **не** предназначены для долгосрочного хранения или как замена Parquet. Они оптимизированы для **быстрого временного хранения и межпроцессного обмена данными**. Ключевые отличия от Parquet:

| Аспект | Arrow IPC / Feather | Parquet |
|--------|-------------------|---------|
| Основное назначение | Быстрый IPC, кэширование, временное хранение | Долгосрочное аналитическое хранение |
| Сжатие | Необязательное (LZ4, ZSTD) | Всегда рекомендуется |
| Степень сжатия | Умеренная | Высокая (кодирование + сжатие) |
| Скорость чтения | Чрезвычайно быстрая (почти zero-copy) | Быстрая (требуется декодирование) |
| Скорость записи | Очень быстрая | Умеренная (накладные расходы на кодирование) |
| Метаданные/индексы | Минимальные | Богатые (статистика, индексы страниц) |
| Предикатный pushdown | Нет | Да |
| Поддержка экосистемы | Растущая | Повсеместная |

### 6.5 Роль в современной обработке данных

Arrow стал **основой современной обработки данных**, даже когда пользователи не взаимодействуют с ним напрямую:

- **pandas 2.0+**: начиная с pandas 2.0 (апрель 2023), Arrow доступен в качестве бэкенда для DataFrame через `dtype_backend="pyarrow"`. Это обеспечивает значительно лучшую производительность и эффективность использования памяти по сравнению с традиционным бэкендом NumPy, особенно для данных с большим количеством строк и типов с поддержкой null.

```python
import pandas as pd

# Использование бэкенда Arrow для повышения производительности
df = pd.read_parquet("data.parquet", dtype_backend="pyarrow")
```

- **Polars**: библиотека DataFrame Polars (написана на Rust) использует Arrow в качестве нативного формата в памяти. Это одна из причин, по которой Polars значительно быстрее традиционного pandas для многих операций.

- **DuckDB**: DuckDB использует Arrow для внутреннего представления данных и обеспечивает интеграцию без копирования с Python через PyArrow. Результат запроса DuckDB может быть использован как таблица Arrow без какого-либо копирования данных.

```python
import duckdb

# Результат запроса DuckDB как таблица Arrow -- zero copy
arrow_table = duckdb.sql("""
    SELECT user_id, COUNT(*) as event_count
    FROM read_parquet('events/*.parquet')
    GROUP BY user_id
""").arrow()
```

- **Spark**: Apache Spark использует Arrow для эффективной передачи данных между JVM-Spark и Python (PySpark). При включённом Arrow pandas UDF обмениваются данными через Arrow вместо сериализации pickle, обеспечивая ускорение в 10-100 раз.

- **Flight RPC**: Arrow Flight -- это высокопроизводительный RPC-фреймворк, построенный на Arrow и gRPC, который обеспечивает сетевую передачу данных Arrow на скоростях, близких к теоретическому максимуму сетевого оборудования.

### 6.6 Как Arrow связан с Parquet

Arrow и Parquet **дополняют** друг друга, а не конкурируют:

- **Parquet** -- формат хранения (данные в покое). Он оптимизирован для диска: максимальное сжатие, минимальный объём хранения, богатые метаданные для оптимизации запросов.
- **Arrow** -- формат обработки (данные в движении / в памяти). Он оптимизирован для CPU: кэш-дружественная раскладка, векторизованные операции, обмен без копирования.

Типичный поток данных:

```
Диск (Parquet) --> Чтение + Декодирование --> Память (Arrow) --> Обработка --> Кодирование + Запись --> Диск (Parquet)
```

Библиотека `pyarrow` предоставляет высокооптимизированные процедуры для конвертации между Parquet и Arrow, делая этот цикл очень эффективным.

---

## 7. Обзор алгоритмов сжатия

Сжатие играет критическую роль в хранении и обработке больших данных. Выбор алгоритма сжатия влияет на стоимость хранения, пропускную способность ввода-вывода и нагрузку на процессор. Ниже приведён обзор наиболее часто используемых алгоритмов в экосистеме больших данных.

### 7.1 Snappy

- **Разработан**: Google (первоначально назывался "Zippy")
- **Философия**: скорость важнее степени сжатия.
- **Типичная степень**: 2-4x
- **Скорость**: очень быстрое сжатие и декомпрессия. Рассчитан на работу со скоростью примерно 250 МБ/с при сжатии и 500 МБ/с при декомпрессии на ядро CPU.
- **Разделяемость**: сам Snappy не является разделяемым, но файловые форматы (Parquet, ORC, Avro) применяют Snappy на уровне блоков, делая общий файл разделяемым.
- **Сценарий использования**: выбор по умолчанию для многих систем обработки больших данных (Parquet, HBase). Лучший выбор, когда CPU является узким местом или критична задержка.

### 7.2 GZIP / ZLIB (Deflate)

- **Стандарт**: RFC 1952 (GZIP) / RFC 1950 (ZLIB), оба основаны на алгоритме DEFLATE (RFC 1951).
- **Философия**: хорошая степень сжатия при разумной скорости.
- **Типичная степень**: 5-8x
- **Скорость**: умеренная скорость сжатия (~20-50 МБ/с), умеренная скорость декомпрессии (~200-300 МБ/с). Сжатие значительно медленнее декомпрессии.
- **Разделяемость**: файлы GZIP **не** являются разделяемыми. Однако при использовании GZIP внутри файлового формата (блоки Avro, страницы Parquet) файл остаётся разделяемым на уровне формата.
- **Сценарий использования**: хороший выбор по умолчанию, когда важна стоимость хранения и данные записываются один раз и читаются многократно (накладные расходы на запись амортизируются).

### 7.3 LZ4

- **Разработан**: Yann Collet
- **Философия**: максимальная скорость при приемлемом сжатии.
- **Типичная степень**: 2-3x
- **Скорость**: самый быстрый алгоритм общего назначения. Скорость декомпрессии превышает 1 ГБ/с на ядро на современном оборудовании.
- **Разделяемость**: фреймы LZ4 не являются разделяемыми, но применение на уровне блоков внутри файловых форматов сохраняет разделяемость.
- **Сценарий использования**: обработка в реальном времени, кэширование в памяти, сценарии, где затраты CPU на декомпрессию должны быть минимизированы. Используется в Arrow IPC в качестве сжатия по умолчанию.

### 7.4 Zstandard (ZSTD)

- **Разработан**: Yann Collet (также создатель LZ4) в Facebook
- **Философия**: лучшее из обоих миров -- степень сжатия, близкая к GZIP, при скорости, близкой к Snappy.
- **Типичная степень**: 4-7x
- **Скорость**: скорость сжатия настраивается через уровни (1-22). На уровне 1 работает почти так же быстро, как Snappy, с лучшей степенью. На уровне 19+ приближается к степеням LZMA, но значительно быстрее.
- **Разделяемость**: не является разделяемым по своей природе, но применение на уровне блоков сохраняет разделяемость.
- **Сценарий использования**: всё чаще **рекомендуемый вариант по умолчанию** для большинства нагрузок на больших данных. Поддерживается Parquet, ORC, Avro и Arrow.

### 7.5 BZip2

- **Философия**: максимальная степень сжатия.
- **Типичная степень**: 6-10x
- **Скорость**: очень медленный -- как сжатие, так и декомпрессия (~10-30 МБ/с).
- **Разделяемость**: да -- BZip2 нативно разделяем по блокам, что уникально среди форматов сжатия. Это делало его исторически важным в Hadoop MapReduce.
- **Сценарий использования**: архивное хранение, где стоимость хранения преобладает и данные редко читаются. Всё чаще заменяется ZSTD на высоких уровнях сжатия.

### 7.6 Сравнительная таблица сжатия

| Алгоритм | Степень сжатия | Скорость сжатия | Скорость декомпрессии | Разделяемость | Лучше всего для |
|----------|:-:|:-:|:-:|:-:|------|
| **Snappy** | 2-4x | Очень быстрая | Очень быстрая | Нет* | Низкая задержка, CPU-ограниченные нагрузки |
| **LZ4** | 2-3x | Наиболее быстрая | Наиболее быстрая | Нет* | Реальное время, в памяти, потоковая обработка |
| **GZIP/ZLIB** | 5-8x | Медленная | Умеренная | Нет* | Однократная запись/многократное чтение, экономия хранения |
| **ZSTD** | 4-7x | Быстрая (настраиваемая) | Быстрая | Нет* | Общего назначения, современный вариант по умолчанию |
| **BZip2** | 6-10x | Очень медленная | Очень медленная | Да | Архивирование, наследие Hadoop |

> \* Не разделяемы как отдельные сжатые файлы. Однако при применении на уровне блоков внутри Avro, Parquet или ORC родительский файловый формат остаётся разделяемым по своим собственным границам блоков/групп строк/страйпов.

### 7.7 Практические рекомендации

1. **По умолчанию используйте ZSTD** для новых конвейеров. Он обеспечивает лучший баланс по всем параметрам.
2. **Используйте Snappy или LZ4**, когда задержка важнее эффективности хранения (аналитика в реальном времени, интерактивные запросы).
3. **Используйте GZIP**, когда необходима максимальная совместимость (некоторые старые системы поддерживают только GZIP) или когда стоимость хранения является основной заботой, а скорость записи не важна.
4. **Избегайте BZip2** в новых системах -- ZSTD на высоких уровнях достигает аналогичных степеней при значительно лучшей скорости.
5. **Всегда сжимайте данные в конвейерах обработки больших данных.** Затраты CPU на декомпрессию почти всегда ниже, чем затраты на ввод-вывод при чтении несжатых данных.

---

## 8. Комплексное сравнение форматов

### 8.1 Матрица возможностей

| Возможность | Parquet | Avro | ORC | CSV | JSON/JSONL | Arrow/Feather |
|-------------|:-------:|:----:|:---:|:---:|:----------:|:-------------:|
| **Модель хранения** | Колоночная (гибрид) | Строчная | Колоночная (гибрид) | Строчная (текст) | Строчная (текст) | Колоночная (память) |
| **Схема** | Встроена (подвал) | Встроена (заголовок) | Встроена (подвал) | Нет | Нет (самоописывающий) | Встроена |
| **Эволюция схемы** | Хорошая | Отличная | Хорошая | Н/П | Н/П | Минимальная |
| **Сжатие** | Встроенное (на уровне страниц) | Встроенное (на уровне блоков) | Встроенное (на уровне потоков) | Только внешнее | Только внешнее | Необязательное (LZ4/ZSTD) |
| **Степень сжатия** | Высокая | Умеренная | Высокая | Очень низкая | Очень низкая | Умеренная |
| **Разделяемость** | Да | Да | Да | Да (без сжатия) | JSONL: Да | Да |
| **Вложенные данные** | Да (Dremel) | Да (union/array/map) | Да (struct/list/map) | Нет | Да (нативные) | Да |
| **Человекочитаемость** | Нет | Нет | Нет | Да | Да | Нет |
| **Типобезопасность** | Строгая | Строгая | Строгая | Нет | Слабая | Строгая |
| **Скорость записи** | Умеренная | Быстрая | Умеренная | Быстрая | Быстрая | Очень быстрая |
| **Скорость чтения (полное сканирование)** | Очень быстрая | Быстрая | Очень быстрая | Медленная | Очень медленная | Чрезвычайно быстрая |
| **Отсечение столбцов** | Да | Нет | Да | Нет | Нет | Да |
| **Предикатный pushdown** | Да | Нет | Да (с фильтрами Блума) | Нет | Нет | Нет |
| **Поддержка ACID** | Через Delta/Iceberg | Нет | Нативная (Hive) | Нет | Нет | Нет |

### 8.2 Резюме лучших сценариев использования

| Формат | Лучший сценарий использования |
|--------|-------------------------------|
| **Parquet** | Аналитические запросы, хранилища данных, долгосрочное колоночное хранение, мультидвижковые среды |
| **Avro** | Сериализация данных, потоковая обработка через Kafka, ETL-загрузка с интенсивной записью, требования к эволюции схемы |
| **ORC** | Аналитика, ориентированная на Hive, ACID-транзакции в Hive, озёра данных на базе HDFS |
| **CSV** | Обмен данными с нетехническими пользователями, интеграция с наследием, простой экспорт, отладка |
| **JSON/JSONL** | Загрузка из API, логирование приложений, исследование полуструктурированных данных, конфигурации |
| **Arrow/Feather** | Аналитика в оперативной памяти, межпроцессный обмен данными, кэширование, обработка в pandas/Polars/DuckDB |

### 8.3 Матрица поддержки экосистемой

| Движок/Инструмент | Parquet | Avro | ORC | CSV | JSON | Arrow |
|-------------------|:-------:|:----:|:---:|:---:|:----:|:-----:|
| Apache Spark | Нативная | Нативная | Нативная | Нативная | Нативная | Нативная (PySpark) |
| Apache Hive | Хорошая | Хорошая | Нативная | Хорошая | Хорошая | Ограниченная |
| Trino (Presto) | Нативная | Хорошая | Нативная | Хорошая | Хорошая | Растущая |
| Apache Flink | Хорошая | Нативная | Хорошая | Хорошая | Нативная | Растущая |
| Apache Kafka | Ограниченная | Нативная | Ограниченная | Ограниченная | Нативная | Ограниченная |
| DuckDB | Нативная | Ограниченная | Ограниченная | Нативная | Нативная | Нативная |
| Polars | Нативная | Ограниченная | Ограниченная | Нативная | Нативная | Нативная |
| pandas | Нативная (PyArrow) | Через библиотеку | Ограниченная | Нативная | Нативная | Нативная |
| AWS Athena | Нативная | Нативная | Нативная | Нативная | Нативная | Н/Д |
| Google BigQuery | Нативная | Нативная | Нативная | Нативная | Нативная | Н/Д |

### 8.4 Блок-схема принятия решений (текстовая)

При выборе формата файлов для конвейера обработки больших данных рассмотрите следующий процесс принятия решений:

1. **Данные находятся на границе системы (загрузка/экспорт)?**
   - Загрузка из API? --> Начните с **JSON/JSONL**, конвертируйте в колоночный формат как можно скорее.
   - Получение от партнёров/поставщиков? --> Ожидайте **CSV**, конвертируйте в колоночный формат как можно скорее.
   - Запись в Kafka? --> Используйте **Avro** с Schema Registry.

2. **Данные в пути (потоковая обработка, передача сообщений, IPC)?**
   - Сообщения Kafka? --> **Avro**.
   - Межпроцессный обмен данными (Python/Rust/Java)? --> **Arrow**.
   - Временное кэширование между этапами конвейера? --> **Arrow/Feather**.

3. **Данные в покое (озеро данных, хранилище данных)?**
   - Мультидвижковая среда (Spark, Trino, DuckDB и т.д.)? --> **Parquet**.
   - Только Hive с требованиями ACID? --> **ORC**.
   - Нужны гарантии эволюции схемы на уровне сырых данных? --> **Avro** для сырых, **Parquet** для обработанных.

4. **Основная нагрузка аналитическая (SELECT нескольких столбцов, агрегация, фильтрация)?**
   - Да --> **Parquet** или **ORC** (колоночные).
   - Нет, обработка целых записей --> **Avro** (строчный).

---

## 9. Основные выводы

1. **Не существует универсально "лучшего" формата файлов.** Правильный выбор зависит от конкретной нагрузки, экосистемы и требований. Хорошо спроектированный конвейер данных часто использует несколько форматов на разных этапах.

2. **Паттерн медальонной архитектуры** (Bronze/Silver/Gold) обычно соответствует форматам:
   - **Bronze (сырые данные)**: Avro или JSON (сохранение исходных данных, приоритет скорости записи и гибкости схемы).
   - **Silver (очищенные данные)**: Parquet (конвертация в колоночный формат для эффективной обработки).
   - **Gold (курированные данные)**: Parquet (оптимизированный с партиционированием, сортировкой и компактификацией).

3. **Строчно-ориентированные форматы (Avro) оптимизированы для записи**; колоночные форматы (Parquet, ORC) оптимизированы для чтения. Большинство аналитических нагрузок ориентированы на чтение, поэтому колоночные форматы доминируют в хранилищах данных.

4. **Текстовые форматы (CSV, JSON) следует рассматривать как форматы обмена**, а не хранения. Конвертируйте в бинарные форматы как можно раньше в конвейере.

5. **Arrow -- будущее обработки данных в оперативной памяти.** Понимание Arrow важно не потому, что вы будете хранить данные в формате Arrow, а потому, что Arrow является внутренним движком pandas 2.0+, Polars, DuckDB и многих других современных инструментов.

6. **Всегда сжимайте данные.** Затраты CPU на декомпрессию почти всегда ниже, чем затраты на ввод-вывод при чтении несжатых данных. Для новых систем используйте ZSTD по умолчанию.

7. **ORC и Parquet функционально схожи** для большинства нагрузок. Выбирайте на основе совместимости с экосистемой, а не теоретических различий в производительности. Если вы не привязаны к Hive, более широкая поддержка экосистемы Parquet обычно побеждает.

8. **Управление схемой имеет значение.** Форматы с сильной поддержкой схем (Avro, Parquet, ORC) выявляют проблемы качества данных на ранних стадиях. Форматы без схем (CSV, JSON) перекладывают проблему на потребителей, увеличивая хрупкость системы.

---

## 10. Литература и дополнительные материалы

- Спецификация Apache Avro: https://avro.apache.org/docs/current/specification/
- Спецификация Apache ORC: https://orc.apache.org/specification/
- Спецификация Apache Arrow: https://arrow.apache.org/docs/format/Columnar.html
- Формат Apache Parquet: https://parquet.apache.org/documentation/latest/
- Confluent Schema Registry: https://docs.confluent.io/platform/current/schema-registry/
- Zstandard (RFC 8878): https://facebook.github.io/zstd/
- "The Design and Implementation of Modern Column-Oriented Database Systems" (Abadi et al.)
- "Dremel: Interactive Analysis of Web-Scale Datasets" (Melnik et al., 2010)
- Wes McKinney, "Apache Arrow and the Future of Data Frames" (блог-пост)
- Databricks, "Delta Lake: High-Performance ACID Table Storage" (whitepaper)
