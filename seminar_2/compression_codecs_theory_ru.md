# Кодеки сжатия и алгоритмы для обработки больших данных

> **Семинар 2 -- Подробный разбор алгоритмов сжатия**
> Продолжительность: ~20 минут лекционного материала (или справочник для самостоятельного изучения)
> Необходимые знания: базовое понимание двоичного представления данных, знакомство с форматами файлов из `columnar_and_row_formats_theory.md`
> Сопутствующий материал: обзор кодеков в разделе 7 документа `columnar_and_row_formats_theory.md`

---

## Содержание

1. [Введение в сжатие данных](#1-введение-в-сжатие-данных)
2. [LZ77 / LZ78 (Фундамент)](#2-lz77--lz78-фундамент)
3. [Deflate (GZIP / ZLIB)](#3-deflate-gzip--zlib)
4. [Snappy](#4-snappy)
5. [LZ4](#5-lz4)
6. [Zstandard (ZSTD)](#6-zstandard-zstd)
7. [BZip2](#7-bzip2)
8. [Run-Length Encoding (RLE)](#8-run-length-encoding-rle)
9. [Словарное кодирование (Dictionary Encoding)](#9-словарное-кодирование-dictionary-encoding)
10. [Практическое сравнение](#10-практическое-сравнение)
11. [Матрица поддержки кодеков](#11-матрица-поддержки-кодеков)

---

## 1. Введение в сжатие данных

### 1.1 Почему сжатие важно в big data

В системах обработки больших данных узким местом почти никогда не является процессор -- им является **ввод-вывод (I/O)**. Чтение данных с диска, передача по сети и запись обратно на порядки медленнее, чем арифметические операции, которые процессор выполняет после получения данных. Рассмотрим пропускную способность различных уровней хранения:

| Операция                              | Пропускная способность |
|---------------------------------------|------------------------|
| Чтение из L1 кэша процессора         | ~1 ТБ/с               |
| Чтение из L3 кэша процессора         | ~200 ГБ/с             |
| Последовательное чтение DDR4 RAM      | ~40 ГБ/с              |
| Последовательное чтение NVMe SSD      | ~3-7 ГБ/с             |
| Последовательное чтение SATA SSD      | ~500 МБ/с             |
| Последовательное чтение HDD           | ~100-200 МБ/с         |
| 10 Gbit Ethernet (датацентр)          | ~1.2 ГБ/с             |
| 1 Gbit Ethernet                       | ~125 МБ/с             |
| Облачное хранилище (S3, GCS)          | ~200-500 МБ/с         |

Когда процессор может распаковывать данные со скоростью 800 МБ/с, а диск отдаёт только 200 МБ/с, сжатие файла размером 1 ГБ до 250 МБ означает: система читает 250 МБ с диска (1.25 секунды) плюс тратит ~0.3 секунды на декомпрессию, итого ~1.55 секунды -- вместо 5 секунд на чтение несжатого файла. **Сжатие ускоряет чтение, а не замедляет его.**

Три ключевые мотивации:

1. **Сокращение ожидания I/O** -- меньше данных нужно прочитать с диска или получить по сети.
2. **Снижение стоимости хранения** -- облачные хранилища тарифицируются за ГБ; 4-кратное сжатие означает 4-кратное снижение затрат.
3. **Увеличение пропускной способности сети** -- при shuffle-операциях в Spark, MapReduce или распределённых соединениях между узлами передаётся меньше данных.

### 1.2 Без потерь (lossless) vs с потерями (lossy)

- **Сжатие с потерями (lossy)** отбрасывает информацию, считающуюся "неважной". Используется для изображений (JPEG), аудио (MP3) и видео (H.264). Распакованный результат -- приближение к оригиналу.
- **Сжатие без потерь (lossless)** гарантирует, что распакованный результат побитово идентичен исходным данным. Это **единственный** вид сжатия, применяемый при обработке данных, потому что потеря даже одного бита в финансовой транзакции, показании датчика или строке лога повредит весь набор данных.

Все алгоритмы, рассмотренные в этом документе, являются **сжатием без потерь**.

### 1.3 Два фундаментальных подхода

Практически все алгоритмы сжатия без потерь относятся к двум большим семействам, которые часто комбинируются:

**Словарное сжатие (семейство LZ)**

Основная идея: если последовательность байтов уже встречалась ранее, не записываем её повторно -- вместо этого записываем короткую ссылку ("вернись на 47 байтов назад и скопируй 12 байтов оттуда"). Алгоритм строит "словарь" ранее виденных паттернов: либо неявно (через скользящее окно по недавним данным, как в LZ77), либо явно (как растущая таблица записей, как в LZ78).

```
Исходная строка:    THE CAT SAT ON THE MAT
                                    ^^^
                                    "THE" уже встречалось на позиции 0!

Сжатый вариант:     THE CAT SAT ON [ссылка: смещение=15, длина=4] MAT
```

**Энтропийное кодирование (Хаффман, арифметическое, FSE)**

Основная идея: присвоить более короткие битовые коды символам, которые встречаются часто, и более длинные коды символам, которые встречаются редко. Если буква "E" встречается 100 раз, а "Z" -- один раз, дадим "E" 2-битный код, а "Z" -- 12-битный. Это использует статистическую неравномерность данных.

```
Частоты символов:  A=45%  B=30%  C=15%  D=10%

Фиксированная длина (2 бита каждый):  A=00  B=01  C=10  D=11
Переменная длина (Хаффман):           A=0   B=10  C=110 D=111

Фиксированная:  100 символов * 2 бита = 200 бит
Хаффман:        45*1 + 30*2 + 15*3 + 10*3 = 45 + 60 + 45 + 30 = 180 бит
Экономия:       10%
```

### 1.4 Компромисс: степень сжатия vs скорость

Каждый кодек занимает определённое положение в пространстве компромиссов:

```
Степень сжатия (выше = лучше сжатие)
      ^
      |
  5.0 |                                      * BZip2
      |                                * ZSTD-19
  4.0 |                          * ZSTD-9
      |                    * GZIP-9
  3.0 |              * GZIP-1
      |        * ZSTD-1
  2.5 |  * Snappy
      |  * LZ4
  2.0 |
      +----------------------------------------> Скорость декомпрессии (выше = быстрее)
         100    500    1000   2000   3000  4000  МБ/с
```

Ключевое правило: **лучшее сжатие = больше времени на сжатие/декомпрессию**. Но разные алгоритмы достигают разных точек на этой кривой. ZSTD на низких уровнях сжимает лучше Snappy при сопоставимой скорости -- именно поэтому он считается лучшим универсальным кодеком.

---

## 2. LZ77 / LZ78 (Фундамент)

Алгоритмы Лемпеля-Зива (1977 и 1978 годов) -- это фундамент, на котором построены практически все современные кодеки сжатия: GZIP, Snappy, LZ4, ZSTD. Понимание LZ77/LZ78 -- ключ к пониманию всех остальных алгоритмов.

### 2.1 LZ77: механизм скользящего окна

**Принцип работы:**

LZ77 использует **скользящее окно (sliding window)**, состоящее из двух частей:

1. **Буфер поиска (search buffer)** -- уже обработанные данные (обычно 4-32 КБ). Здесь алгоритм ищет совпадения.
2. **Буфер опережения (lookahead buffer)** -- ещё не обработанные данные. Алгоритм берёт из него строку и ищет её в буфере поиска.

Выходные данные -- последовательность троек **(смещение, длина, следующий_символ)**:
- **смещение** -- на сколько позиций назад от текущей позиции начинается совпадение
- **длина** -- сколько символов совпало
- **следующий_символ** -- первый символ после совпадения (нужен для продвижения вперёд)

Если совпадение не найдено, выдаётся тройка `(0, 0, символ)` -- литерал.

#### Пошаговый пример: сжатие строки `AABABCABCD`

Для наглядности возьмём маленькое окно. Буфер поиска -- все уже обработанные символы. Буфер опережения -- остаток строки.

```
Строка: A A B A B C A B C D
Индекс: 0 1 2 3 4 5 6 7 8 9
```

**Шаг 1:** Позиция = 0. Буфер поиска пуст. Буфер опережения: `AABABCABCD`

```
Буфер поиска:  [          ]  Буфер опережения: [A|ABABCABCD]
                                                 ^
Совпадение не найдено (буфер поиска пуст).
Выход: (0, 0, 'A')
Позиция сдвигается на 1.
```

**Шаг 2:** Позиция = 1. Буфер поиска: `A`. Буфер опережения: `ABABCABCD`

```
Буфер поиска:  [A]  Буфер опережения: [A|BABCABCD]
                ^---------------------> ^
Найдено совпадение: 'A' на смещении 1, длина 1.
Следующий символ после совпадения: 'B'.
Выход: (1, 1, 'B')
Позиция сдвигается на 2 (длина совпадения + 1).
```

**Шаг 3:** Позиция = 3. Буфер поиска: `AAB`. Буфер опережения: `ABCABCD`

```
Буфер поиска:  [A A B]  Буфер опережения: [A B C|ABCD]
                ^ ^-----------------------^-^
Ищем самое длинное совпадение:
  'A'  -- совпадает на смещении 3 (длина 1) и на смещении 2 (длина 1)
  'AB' -- совпадает на смещении 2 (длина 2)
  'ABC' -- нет совпадения
Лучшее: смещение=2, длина=2 ('AB')
Следующий символ: 'C'
Выход: (2, 2, 'C')
Позиция сдвигается на 3.
```

**Шаг 4:** Позиция = 6. Буфер поиска: `AABABC`. Буфер опережения: `ABCD`

```
Буфер поиска:  [A A B A B C]  Буфер опережения: [A B C D]
                      ^ ^ ^---------------------> ^ ^ ^
Ищем самое длинное совпадение:
  'A'   -- совпадает (много вариантов)
  'AB'  -- совпадает на смещении 2
  'ABC' -- совпадает на смещении 3 (позиция 3: ABCABCD, нет)
          Проверяем: буфер поиска[3..5] = "ABC" -- совпадение!
          Смещение = 6-3 = 3, длина = 3
  'ABCD'-- нет совпадения (D не встречался)
Лучшее: смещение=3, длина=3 ('ABC')
Следующий символ: 'D'
Выход: (3, 3, 'D')
Позиция сдвигается на 4. Достигнут конец строки.
```

**Итого, закодированная последовательность:**

```
(0,0,'A')  (1,1,'B')  (2,2,'C')  (3,3,'D')
```

**Результат сжатия:** Исходная строка -- 10 символов. Закодированная -- 4 тройки. Каждая тройка занимает меньше места, чем исходные символы (при использовании компактного двоичного представления).

#### ASCII-диаграмма скользящего окна

```
Шаг 1:  [        ][ A A B A B C A B C D ]   -- буфер поиска пуст
Шаг 2:  [ A      ][ A B A B C A B C D ]     -- обработан 1 символ
Шаг 3:  [ A A B  ][ A B C A B C D ]         -- обработано 3 символа
Шаг 4:  [ A A B A B C ][ A B C D ]          -- обработано 6 символов
Итог:   [ A A B A B C A B C D ][ ]           -- все данные обработаны
```

### 2.2 LZ78: словарный подход без скользящего окна

**Принцип работы:**

В отличие от LZ77, LZ78 не использует скользящее окно. Вместо этого он строит **явный словарь** (таблицу) фраз. Каждая новая запись в словаре -- это предыдущая запись + один новый символ.

Выходные данные -- последовательность пар **(индекс_словаря, символ)**:
- **индекс_словаря** -- номер записи в словаре, которая является префиксом текущей фразы (0 = пустая строка)
- **символ** -- новый символ, добавляемый к этому префиксу

#### Пошаговый пример: сжатие строки `AABABCABCD`

```
Строка: A A B A B C A B C D
Индекс: 0 1 2 3 4 5 6 7 8 9
```

| Шаг | Текущий ввод | Поиск в словаре | Найдено? | Выход | Новая запись в словаре |
|-----|-------------|-----------------|----------|-------|------------------------|
| 1   | `A`         | `A`             | Нет      | (0, 'A') | 1: "A"              |
| 2   | `A`         | `A`             | Да (запись 1) | -- | --                 |
|     | `AB`        | `AB`            | Нет      | (1, 'B') | 2: "AB"             |
| 3   | `A`         | `A`             | Да (запись 1) | -- | --                 |
|     | `AB`        | `AB`            | Да (запись 2) | -- | --                 |
|     | `ABC`       | `ABC`           | Нет      | (2, 'C') | 3: "ABC"            |
| 4   | `A`         | `A`             | Да (запись 1) | -- | --                 |
|     | `AB`        | `AB`            | Да (запись 2) | -- | --                 |
|     | `ABC`       | `ABC`           | Да (запись 3) | -- | --                 |
|     | `ABCD`      | `ABCD`          | Нет      | (3, 'D') | 4: "ABCD"           |

**Состояние словаря на каждом шаге:**

```
После шага 1:  {1: "A"}
После шага 2:  {1: "A", 2: "AB"}
После шага 3:  {1: "A", 2: "AB", 3: "ABC"}
После шага 4:  {1: "A", 2: "AB", 3: "ABC", 4: "ABCD"}
```

**Закодированная последовательность:**

```
(0,'A')  (1,'B')  (2,'C')  (3,'D')
```

Обратите внимание: словарь растёт, и каждая новая запись содержит всё более длинные фразы. Это позволяет LZ78 эффективно сжимать повторяющиеся паттерны без необходимости хранить скользящее окно.

### 2.3 Сравнение LZ77 и LZ78

| Характеристика        | LZ77                                         | LZ78                                     |
|-----------------------|----------------------------------------------|------------------------------------------|
| Словарь               | Неявный (скользящее окно)                    | Явный (растущая таблица)                 |
| Использование памяти  | Фиксированное (размер окна)                 | Растущее (словарь может стать большим)   |
| Поиск совпадений      | В буфере поиска (линейный или с хешами)      | По хеш-таблице словаря                   |
| Основа для            | Deflate, Snappy, LZ4, ZSTD                  | LZW (GIF, TIFF, compress)               |
| Скорость              | Зависит от размера окна                      | Обычно быстрее на сжатие                |
| Степень сжатия        | Выше при большом окне                        | Хорошая для структурированных данных     |
| Патенты               | Свободен                                     | LZW был запатентован (Unisys), сейчас истёк |

**Важный факт:** практически все современные кодеки (GZIP, Snappy, LZ4, ZSTD) основаны на LZ77, а не на LZ78. Причина -- LZ77 с фиксированным окном проще в реализации, предсказуемее по памяти и лучше комбинируется с энтропийным кодированием.

---

## 3. Deflate (GZIP / ZLIB)

### 3.1 Двухэтапный конвейер

Deflate -- это алгоритм сжатия, используемый в GZIP и ZLIB. Он комбинирует два этапа:

```
Исходные данные
     |
     v
+------------------+
| Этап 1: LZ77     |  -- находит повторяющиеся последовательности,
| (словарное       |     заменяет их ссылками (смещение, длина)
|  сжатие)         |
+------------------+
     |
     v
Промежуточный поток: литералы + ссылки (смещение, длина)
     |
     v
+------------------+
| Этап 2: Хаффман  |  -- кодирует литералы и длины/смещения
| (энтропийное     |     короткими битовыми кодами
|  кодирование)    |
+------------------+
     |
     v
Сжатый поток битов
```

Ключевая идея: LZ77 убирает **повторения** в данных, а кодирование Хаффмана убирает **статистическую избыточность** (неравномерность частот символов). Комбинация двух этапов даёт значительно лучшее сжатие, чем каждый этап по отдельности.

### 3.2 Кодирование Хаффмана -- подробный пример

Кодирование Хаффмана (David Huffman, 1952) -- один из самых фундаментальных алгоритмов в теории информации. Рассмотрим его работу пошагово.

#### Задача

Дана строка: `AABBCBAAACBBAAAC`

**Шаг 1: Подсчёт частот символов**

```
A: 8 раз  (50.0%)
B: 5 раз  (31.25%)
C: 3 раза (18.75%)
```

**Шаг 2: Построение дерева Хаффмана**

Алгоритм использует **очередь с приоритетами (min-heap)**: на каждом шаге извлекаем два узла с наименьшими частотами, объединяем их в новый узел, частота которого равна сумме частот потомков.

Начальная очередь (отсортирована по частоте):

```
Очередь: [C:3] [B:5] [A:8]
```

**Итерация 1:** Извлекаем C:3 и B:5, создаём узел CB:8

```
    [CB:8]
    /    \
  C:3   B:5

Очередь: [A:8] [CB:8]
```

**Итерация 2:** Извлекаем A:8 и CB:8, создаём корень:16

```
       [Корень:16]
       /          \
     A:8        [CB:8]
                /    \
              C:3   B:5

Очередь: [Корень:16]  -- остался один узел, дерево построено.
```

**Шаг 3: Присвоение кодов**

Идём от корня: левая ветвь = `0`, правая ветвь = `1`.

```
          [16]
         0/   \1
        A:8   [8]
             0/  \1
           C:3   B:5
```

Результирующие коды:

| Символ | Частота | Код Хаффмана | Длина (бит) |
|--------|---------|-------------|-------------|
| A      | 8       | `0`         | 1           |
| B      | 5       | `11`        | 2           |
| C      | 3       | `10`        | 2           |

**ASCII-art дерева Хаффмана:**

```
              *
             / \
            /   \
           /     \
         [A]     *
         (0)    / \
               /   \
             [C]  [B]
             (10) (11)
```

**Шаг 4: Кодирование строки**

```
Исходная:   A  A  B  B  C  B  A  A  A  C  B  B  A  A  A  C
Код:        0  0  11 11 10 11 0  0  0  10 11 11 0  0  0  10

Закодированный битовый поток: 0 0 11 11 10 11 0 0 0 10 11 11 0 0 0 10
                             = 0011111000101100010 (19 бит)
```

**Сравнение:**

```
Фиксированная длина (2 бита на символ): 16 * 2 = 32 бита
Кодирование Хаффмана:                  8*1 + 5*2 + 3*2 = 24 бита
                                        (или 19 бит для конкретной строки)
Экономия:                              25%
```

**Шаг 5: Свойство префиксных кодов**

Коды Хаффмана являются **префиксными кодами**: ни один код не является префиксом другого. Это позволяет однозначно декодировать битовый поток слева направо без разделителей:

```
Битовый поток: 0011111000101100010

Декодирование:
  0      -> A (ни один код не начинается с "0" кроме A)
  0      -> A
  11     -> B (0 -> нет, 10 -> нет, 11 -> B)
  11     -> B
  10     -> C
  11     -> B
  0      -> A
  ...и так далее
```

Если бы коды были `A=0, B=01, C=10`, то поток `001...` можно было бы декодировать как `AA1...` или `AB...` -- неоднозначность. Префиксные коды эту проблему исключают.

### 3.3 Как два этапа комбинируются

В Deflate LZ77 генерирует поток из трёх типов элементов:
- **Литералы** (байты, которые не нашли совпадения): значения 0-255
- **Длины совпадений**: значения 257-285 (длины 3-258)
- **Маркер конца блока**: значение 256

Все литералы и длины кодируются **одним деревом Хаффмана** (до 286 символов). Смещения кодируются **вторым деревом Хаффмана** (до 30 символов).

```
Поток LZ77:  лит('T') лит('H') лит('E') лит(' ') лит('C') лит('A') лит('T')
             лит(' ') лит('S') лит('A') лит('T') лит(' ') лит('O') лит('N')
             лит(' ') ссылка(смещ=15, дл=4) лит('M') лит('A') лит('T') конец

Дерево 1 (литералы/длины):  T=10, H=1100, E=1101, ' '=01, C=11100, ...
Дерево 2 (смещения):        15=0101, ...

Битовый поток: [биты дерева 1][биты дерева 2][закодированные данные]
```

### 3.4 GZIP vs ZLIB vs raw Deflate

Все три используют **один и тот же** алгоритм сжатия Deflate. Разница только в обёртке:

| Формат       | Заголовок          | Контрольная сумма    | Применение                    |
|-------------|--------------------|-----------------------|-------------------------------|
| Raw Deflate  | Нет                | Нет                   | Внутри других форматов        |
| ZLIB         | 2 байта (CMF+FLG) | Adler-32 (4 байта)   | PNG, HTTP, внутренний формат  |
| GZIP         | 10+ байт (magic number, метаданные) | CRC-32 (4 байта) | Файлы `.gz`, HTTP, Hadoop |

```
GZIP-файл: [ID1=1F][ID2=8B][CM=08][FLG][MTIME 4B][XFL][OS]
            [... сжатые данные Deflate ...]
            [CRC-32 4B][Исходный размер 4B]
```

### 3.5 Уровни сжатия (1-9)

GZIP/ZLIB поддерживают уровни сжатия от 1 до 9. Что они контролируют:

| Уровень | Размер окна поиска LZ77 | Стратегия поиска | Скорость     | Степень сжатия |
|---------|--------------------------|------------------|-------------|----------------|
| 1       | Минимальный              | Жадный поиск, первое совпадение | Быстро  | Низкая   |
| 4-5     | Средний                  | Ленивый поиск (lazy matching)   | Средне  | Средняя  |
| 6       | По умолчанию             | Ленивый поиск                   | Баланс  | Хорошая  |
| 9       | Максимальный             | Исчерпывающий поиск лучшего совпадения | Медленно | Максимальная |

**Lazy matching** -- хитрость Deflate: найдя совпадение длины L на текущей позиции, алгоритм проверяет, нет ли более длинного совпадения на **следующей** позиции. Если есть, текущий символ выдаётся как литерал, а более длинное совпадение используется вместо короткого.

### 3.6 Когда использовать

- **Архивные (холодные) данные** -- данные, которые пишутся один раз и читаются редко. Время сжатия не критично, а экономия на хранении велика.
- **HTTP-передача** -- GZIP -- стандарт де-факто для HTTP Content-Encoding.
- **Hadoop/MapReduce** -- исторически GZIP широко использовался, но сейчас вытесняется ZSTD.
- **Не рекомендуется** для real-time обработки и стриминга из-за относительно медленной декомпрессии.

---

## 4. Snappy

### 4.1 Общее описание

Snappy (ранее известный как Zippy) разработан в Google. Основной приоритет -- **скорость**, а не максимальная степень сжатия. Snappy на порядок быстрее GZIP при декомпрессии, но сжимает на 20-30% хуже.

Философия проектирования:
- Скорость декомпрессии ~1700 МБ/с (при GZIP ~400 МБ/с)
- Скорость сжатия ~500 МБ/с (при GZIP ~50-100 МБ/с)
- Степень сжатия ~1.5-2.5x (при GZIP ~3-5x)

### 4.2 Как работает: упрощённый LZ77

Snappy использует **упрощённый вариант LZ77** без энтропийного кодирования (второго этапа Deflate). Выходной поток состоит из двух типов элементов:

1. **Литералы (literal)** -- последовательность байтов, скопированная как есть
2. **Копии (copy / back-reference)** -- ссылка на ранее виденные данные (смещение + длина)

Отсутствие энтропийного кодирования -- главная причина скорости Snappy. Вместо построения деревьев Хаффмана и побитовых операций, Snappy работает с целыми байтами.

### 4.3 Формат блока: тег-байт (tag byte)

Каждый элемент в сжатом потоке Snappy начинается с **тег-байта**, младшие 2 бита которого определяют тип элемента:

```
Тег-байт:  [7 6 5 4 3 2 | 1 0]
                         |  ^-- тип элемента (2 бита)
                         ^---- данные, зависящие от типа
```

#### Тип 00: Литерал

```
Тег:  [длина-1 (6 бит) | 0 0]

Если длина-1 < 60:
  Длина литерала = (тег >> 2) + 1
  За тег-байтом следуют байты литерала.

Если длина-1 = 60:  за тегом следует 1 байт длины
Если длина-1 = 61:  за тегом следуют 2 байта длины
Если длина-1 = 62:  за тегом следуют 3 байта длины
Если длина-1 = 63:  за тегом следуют 4 байта длины

Пример: литерал "CAT" (3 байта)
  Длина-1 = 2, тип = 00
  Тег-байт: [0 0 0 0 1 0 | 0 0] = 0x08
  Полный вывод: 08 43 41 54
                ^^-тег ^^ ^^ ^^-данные ('C','A','T')
```

#### Тип 01: Копия с 1-байтовым смещением (близкая ссылка)

```
Тег:  [смещ[10:8] (3 бита) | длина-4 (3 бита) | 0 1]
      + 1 дополнительный байт: смещение[7:0]

Длина = (тег >> 2) & 0x07 + 4  (от 4 до 11)
Смещение = ((тег >> 5) << 8) | следующий_байт  (от 1 до 2047)

Пример: копия 6 байтов со смещением 15
  Длина-4 = 2, смещ[10:8] = 0, смещ[7:0] = 15
  Тег-байт: [0 0 0 | 0 1 0 | 0 1] = 0x09
  Второй байт: 0x0F
  Полный вывод: 09 0F
```

#### Тип 10: Копия с 2-байтовым смещением (дальняя ссылка)

```
Тег:  [длина-1 (6 бит) | 1 0]
      + 2 дополнительных байта: смещение (little-endian)

Длина: от 1 до 64
Смещение: от 1 до 65535
```

#### Тип 11: Копия с 4-байтовым смещением (очень дальняя ссылка)

```
Тег:  [длина-1 (6 бит) | 1 1]
      + 4 дополнительных байта: смещение (little-endian)

Длина: от 1 до 64
Смещение: от 1 до 2^32 - 1
```

### 4.4 Пошаговый пример: кодирование строки `THE CAT SAT ON THE MAT`

```
Позиция: 0123456789...
Строка:  THE CAT SAT ON THE MAT
```

**Шаг 1:** Хеш-таблица пуста. Первые символы записываем как литерал:

```
Позиция 0-14: "THE CAT SAT ON " -- литерал (15 байтов)
  Тег: длина-1=14, тип=00 -> тег-байт: 0x38
  Вывод: 38 54 48 45 20 43 41 54 20 53 41 54 20 4F 4E 20
```

**Шаг 2:** Позиция 15: "THE " найдено в хеш-таблице (совпадение с позицией 0, смещение=15, длина=4). Тип 01 (копия).

```
  Тег-байт: 0x01, второй байт: 0x0F
  Вывод: 01 0F
```

**Шаг 3:** Позиция 19: "MAT" -- нет совпадения. Записываем как литерал.

```
  Тег: длина-1=2, тип=00 -> тег-байт: 0x08
  Вывод: 08 4D 41 54
```

**Итоговый сжатый поток (hex):**

```
38 54 48 45 20 43 41 54 20 53 41 54 20 4F 4E 20  -- литерал (15 байт)
01 0F                                              -- копия (4 байта, смещ. 15)
08 4D 41 54                                        -- литерал (3 байта)
Итого: 22 байта -> ~22 байта (на коротких строках сжатие минимально)
```

На реальных данных (килобайты и мегабайты) Snappy находит больше совпадений и достигает степени сжатия 1.5-2.5x.

### 4.5 Почему Snappy такой быстрый

1. **Нет энтропийного кодирования** -- не строит деревья Хаффмана, не выполняет побитовые операции. Работает с целыми байтами.
2. **Простой формат** -- тег-байт определяет тип и длину за одну операцию.
3. **Хеш-таблица вместо полного поиска** -- для поиска совпадений используется хеш-таблица с 4-байтовыми ключами. Каждый 4-байтовый фрагмент хешируется, и проверяется ровно одна позиция (без цепочек).
4. **Оптимизация под современные CPU** -- использует встроенные инструкции копирования памяти, минимизирует ветвления, данные помещаются в L1/L2 кэш.

### 4.6 Использование

- **Parquet** -- кодек по умолчанию (хотя ZSTD рекомендуется для новых проектов)
- **Avro** -- широко используется как кодек для Avro-файлов
- **Google внутренне** -- Bigtable, MapReduce, RPC-вызовы
- **Real-time обработка** -- Kafka, потоковые системы

---

## 5. LZ4

### 5.1 Общее описание

LZ4 -- алгоритм сжатия, разработанный Янном Колле (Yann Collet) в 2011 году. Его главная цель -- **сверхбыстрая декомпрессия**, даже быстрее Snappy. LZ4 спроектирован так, чтобы скорость декомпрессии приближалась к скорости `memcpy()` -- простого копирования памяти.

Характеристики:
- Скорость декомпрессии: ~4000-5000 МБ/с (ограничена пропускной способностью RAM)
- Скорость сжатия: ~700-800 МБ/с
- Степень сжатия: ~2.0-2.5x (чуть лучше Snappy)

### 5.2 Формат токена

LZ4 использует крайне простой формат. Сжатый поток -- последовательность **последовательностей (sequences)**, каждая из которых состоит из:

```
+--------+---...---+--------+--------+---...---+
| Токен  | Литерал | Смещ.L | Смещ.H | Доп.дл. |
| 1 байт | N байт  | 1 байт | 1 байт | 0+ байт |
+--------+---...---+--------+--------+---...---+
```

**Токен-байт** разделён на два полубайта (nibble):

```
Токен:  [старший полубайт | младший полубайт]
         длина литерала     длина совпадения (- 4)
         (4 бита: 0-15)     (4 бита: 0-15)
```

- **Старший полубайт (биты 7-4):** длина литеральной секции. Если = 15, нужно расширение.
- **Младший полубайт (биты 3-0):** длина совпадения минус 4 (минимальная длина совпадения = 4). Если = 15, нужно расширение.

### 5.3 Схема расширения длины

Если значение полубайта = 15, это означает "15 + прочитай следующий байт". Пока читаемый байт = 255, продолжаем складывать:

```
Пример: длина литерала = 280

Старший полубайт = 15 (максимум)
Дополнительные байты: 255, 10
Итого: 15 + 255 + 10 = 280

Пример: длина литерала = 20

Старший полубайт = 15
Дополнительные байты: 5
Итого: 15 + 5 = 20

Пример: длина литерала = 7

Старший полубайт = 7 (помещается, доп. байты не нужны)
```

### 5.4 Пошаговый пример: кодирование строки `ABCD ABCD EFGH ABCD`

```
Позиция:  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18
Строка:   A  B  C  D     A  B  C  D     E  F  G  H     A  B  C  D
```

**Шаг 1:** Позиция 0-4. Алгоритм сканирует, хеш-таблица пуста. Совпадения нет. На позиции 5 находим "ABCD" в хеш-таблице (совпадение с позицией 0).

```
Литерал: "ABCD " (5 байтов, позиции 0-4)
Совпадение: "ABCD " (5 байтов, смещение = 5, позиция 5-9)

Токен:
  Длина литерала = 5 -> старший полубайт = 5
  Длина совпадения = 5, но минимум 4, значит: 5 - 4 = 1 -> младший полубайт = 1

  Токен-байт: 0x51 (старший=5, младший=1)

Смещение: 5 (little-endian: 05 00)

Полный вывод (hex): 51 41 42 43 44 20 05 00
                     ^^ ^^^^^^^^^^^^^^^^ ^^^^^
                     токен  литерал       смещение
                     5|1    "ABCD "       5 (LE)
```

**Шаг 2:** Позиция 10-14. Строка "EFGH " не находит совпадения. На позиции 15 находим "ABCD" (совпадение с позицией 0 или 5).

```
Литерал: "EFGH " (5 байтов, позиции 10-14)
Совпадение: "ABCD" (4 байта, смещение = 15, позиция 15-18)

Токен:
  Длина литерала = 5 -> старший полубайт = 5
  Длина совпадения = 4 - 4 = 0 -> младший полубайт = 0

  Токен-байт: 0x50

Смещение: 15 (little-endian: 0F 00)

Полный вывод (hex): 50 45 46 47 48 20 0F 00
                     ^^ ^^^^^^^^^^^^^^^^ ^^^^^
                     токен  литерал       смещение
                     5|0    "EFGH "       15 (LE)
```

**Последняя последовательность** (конец блока): в LZ4 последняя последовательность не содержит совпадения (только литерал, если остались необработанные байты). В нашем случае все данные обработаны.

**Итоговый сжатый поток (hex):**

```
51 41 42 43 44 20 05 00 50 45 46 47 48 20 0F 00

Исходный размер: 19 байт
Сжатый размер:  16 байт
Степень сжатия: 1.19x (на такой короткой строке -- минимальное сжатие)
```

### 5.5 LZ4 vs LZ4 HC (высокая компрессия)

| Характеристика        | LZ4                    | LZ4 HC (High Compression) |
|-----------------------|------------------------|----------------------------|
| Скорость сжатия        | ~700 МБ/с             | ~50-100 МБ/с               |
| Скорость декомпрессии  | ~4000 МБ/с            | ~4000 МБ/с (та же!)        |
| Степень сжатия         | ~2.0x                 | ~2.7-3.0x                  |
| Алгоритм поиска        | Хеш-таблица (1 проба) | Двоичное дерево / цепочки  |
| Уровни                 | Нет (один режим)      | 1-12                       |

Ключевой факт: **скорость декомпрессии LZ4 HC такая же, как у обычного LZ4**, потому что формат выходных данных одинаковый. HC только тратит больше времени на поиск лучших совпадений при сжатии.

### 5.6 Использование

- **Стриминг и real-time обработка** -- когда скорость декомпрессии критична
- **In-memory кэширование** -- сжатие данных в памяти для экономии RAM
- **Формат Feather** (Apache Arrow) -- LZ4 по умолчанию
- **Базы данных** -- MySQL InnoDB, ClickHouse, RocksDB
- **Файловые системы** -- ZFS поддерживает LZ4 как кодек сжатия по умолчанию

---

## 6. Zstandard (ZSTD)

### 6.1 Общее описание

Zstandard (ZSTD) создан Янном Колле (Yann Collet, автор LZ4) в Facebook (Meta) в 2015-2016 годах. ZSTD -- это алгоритм, который **доминирует** в пространстве компромиссов "степень сжатия vs скорость":

- На низких уровнях (1-3) -- сжимает лучше Snappy/LZ4 при сопоставимой скорости
- На средних уровнях (4-9) -- сжимает как GZIP, но в 3-5 раз быстрее
- На высоких уровнях (10-22) -- сжимает лучше BZip2

ZSTD считается **лучшим универсальным кодеком** на сегодняшний день.

### 6.2 Трёхэтапная архитектура

```
Исходные данные
     |
     v
+------------------+
| Этап 1: LZ77     |  Поиск совпадений (словарное сжатие)
+------------------+
     |
     v
Три потока: [литералы] [смещения] [длины совпадений]
     |            |           |
     v            v           v
+----------+ +----------+ +----------+
| Хаффман  | |   FSE    | |   FSE    |
| (этап 2a)| | (этап 2b)| | (этап 2c)|
+----------+ +----------+ +----------+
     |            |           |
     v            v           v
+----------------------------------------+
|         Сжатый блок ZSTD               |
+----------------------------------------+
```

- **Литералы** кодируются **кодированием Хаффмана** (быстрое декодирование, хорошее сжатие для байтовых данных)
- **Смещения и длины совпадений** кодируются **FSE (Finite State Entropy)** -- более продвинутым энтропийным кодированием

### 6.3 FSE (Finite State Entropy) -- доступное объяснение

FSE -- это метод энтропийного кодирования, изобретённый Янном Колле. Он по сжатию приближается к **арифметическому кодированию** (теоретический оптимум), но работает значительно быстрее, потому что использует **предвычисленные таблицы переходов** вместо арифметических операций с высокой точностью.

#### Интуиция

Представьте автомат (конечный автомат -- finite state machine) с набором состояний. Каждое состояние -- это число. При кодировании каждого символа автомат переходит из текущего состояния в новое, определяемое таблицей переходов. Эта таблица построена так, чтобы частым символам соответствовали переходы с большим числом вариантов (= меньше бит на кодирование), а редким -- с малым числом вариантов (= больше бит).

#### Упрощённый пример

Допустим, алфавит: `{A, B}`, частоты: `A=75%, B=25%`.

Таблица состояний FSE (упрощённая, 8 состояний):

```
Размер таблицы: 8 (степень двойки, определяет точность)
Распределение ячеек: A получает 6 ячеек (75% * 8), B получает 2 ячейки (25% * 8)

Таблица кодирования:
+----------+---------+-------------------+-----------------+
| Состояние| Символ  | Выходные биты     | Новое состояние |
+----------+---------+-------------------+-----------------+
| 0        | A       | -                 | 4               |
| 1        | A       | -                 | 5               |
| 2        | B       | 0 (1 бит)         | 4               |
| 3        | A       | 0 (1 бит)         | 4               |
| 4        | A       | 0 (1 бит)         | 5               |
| 5        | A       | 1 (1 бит)         | 4               |
| 6        | B       | 1 (1 бит)         | 5               |
| 7        | A       | 10 (2 бита)       | 5               |
+----------+---------+-------------------+-----------------+
```

Кодирование строки `AABA`:
1. Начальное состояние: 0
2. Кодируем `A`: состояние 0, символ A -> выход: ничего, новое состояние: 4
3. Кодируем `A`: состояние 4, символ A -> выход: `0`, новое состояние: 5
4. Кодируем `B`: ? (ищем состояние для B при текущем состоянии)
   ... и так далее

Ключевое свойство: символ A (частый) в среднем требует ~0.4 бита, символ B (редкий) -- ~2 бита. Это приближается к теоретическому пределу (энтропия Шеннона: -0.75*log2(0.75) - 0.25*log2(0.25) = 0.81 бита на символ).

**Преимущество FSE перед кодированием Хаффмана:** Хаффман не может присвоить символу код короче 1 бита. Если символ встречается в 90% случаев, Хаффман всё равно потратит 1 бит, тогда как FSE потратит ~0.15 бита (в среднем). Для метаданных (смещений, длин), где распределение сильно неравномерное, FSE даёт значительно лучшее сжатие.

### 6.4 Тренировка словарей (dictionary training)

Уникальная возможность ZSTD -- **обучение словаря** на множестве малых документов одного типа.

Проблема: LZ77 ищет совпадения в уже обработанных данных текущего файла. Если файл маленький (например, JSON-объект в 200 байтов), окно поиска почти пусто, и сжатие минимально.

Решение: обучить словарь на тысячах примеров похожих документов. Словарь содержит общие паттерны (ключи JSON, XML-теги, общие значения). При сжатии словарь загружается в буфер поиска **до начала обработки**, так что даже первый байт файла может найти совпадение.

```
Без словаря:
  {"user_id": 12345, "action": "click", "timestamp": "2024-01-15"}
  Степень сжатия: ~1.1x (почти не сжимается)

Со словарём (обученным на 10000 подобных JSON):
  Степень сжатия: ~5-10x (словарь содержит "user_id", "action", "timestamp" и т.д.)
```

Обучение словаря:
```bash
zstd --train samples/* -o my_dictionary.dict          # обучить словарь
zstd --compress -D my_dictionary.dict input.json       # сжать со словарём
```

### 6.5 Уровни сжатия (1-22)

ZSTD поддерживает 22 уровня сжатия. Что меняется:

| Уровни | Название      | Что контролируется                                        | Скорость сжатия | Степень сжатия |
|--------|--------------|-----------------------------------------------------------|-----------------|----------------|
| 1-3    | Быстрые      | Маленькое окно поиска, простой поиск (1 проба в хеш-таблице). Без lazy matching. | ~300-500 МБ/с | ~2.5-2.8x |
| 4-6    | Средние       | Увеличенное окно, хеш-цепочки (4-8 проб). Lazy matching включён. | ~100-200 МБ/с | ~2.8-3.1x |
| 7-9    | Хорошие       | Большое окно, двоичное дерево поиска. Агрессивный lazy matching. | ~40-80 МБ/с | ~3.1-3.3x |
| 10-15  | Высокие       | Очень большое окно (до 128 МБ). Оптимальный парсинг (optimal parsing). | ~10-30 МБ/с | ~3.3-3.5x |
| 16-19  | Очень высокие | Максимальное окно. Полный оптимальный парсинг с перебором. | ~3-10 МБ/с | ~3.5-3.7x |
| 20-22  | Ультра        | Самый агрессивный поиск. Многопроходное сжатие. Огромное потребление RAM. | ~1-3 МБ/с | ~3.7-3.9x |

**Оптимальный парсинг (optimal parsing):** на уровнях 10+ ZSTD использует динамическое программирование для нахождения глобально оптимальной последовательности литералов и совпадений, а не жадный/ленивый локальный выбор.

### 6.6 Формат фрейма

```
Фрейм ZSTD:
[Magic: FD 2F B5 28][Frame header 2-14B][Блок 1]...[Блок N (последний)]

Frame header содержит: Window size, Dictionary ID, Content size, Checksum flag.

Каждый блок:
  [Block header 3B: last_block? | block_type | block_size][данные]

Типы блоков: 00=Raw, 01=RLE, 10=Compressed, 11=Зарезервирован
```

### 6.7 Почему ZSTD -- лучший универсальный кодек

1. **Доминирует на кривой компромиссов** -- на каждом уровне скорости ZSTD сжимает лучше альтернатив.
2. **22 уровня** -- один и тот же кодек покрывает сценарии от real-time до архивного хранения.
3. **Тренировка словарей** -- уникальная возможность для малых данных.
4. **Многопоточное сжатие** -- встроенная поддержка.
5. **Зрелая экосистема** -- поддержка в Parquet, Avro, ORC, Kafka, Hadoop, Spark.
6. **Лицензия BSD** -- полностью свободное использование.

---

## 7. BZip2

### 7.1 Общее описание

BZip2 -- алгоритм сжатия, разработанный Джулианом Сьюардом (Julian Seward) в 1996 году. Он использует принципиально другой подход к сжатию, основанный на **преобразовании данных**, а не прямом поиске совпадений.

Конвейер BZip2:

```
Исходные данные
     |
     v
+---------------------------+
| 1. Преобразование         |
|    Берроуза-Уилера (BWT)  |  -- перегруппировывает символы
+---------------------------+
     |
     v
+---------------------------+
| 2. Move-to-Front (MTF)    |  -- заменяет символы индексами
+---------------------------+
     |
     v
+---------------------------+
| 3. Кодирование Хаффмана   |  -- энтропийное сжатие
+---------------------------+
     |
     v
Сжатые данные
```

### 7.2 Преобразование Берроуза-Уилера (BWT) -- подробный пример

Преобразование Берроуза-Уилера (Burrows-Wheeler Transform) -- гениальный алгоритм, который **не сжимает данные**, а **перегруппировывает** их так, чтобы одинаковые символы оказались рядом друг с другом. После перегруппировки данные гораздо лучше поддаются сжатию.

#### Пошаговый пример: строка `BANANA$`

Символ `$` -- маркер конца строки (лексикографически меньше всех остальных символов).

**Шаг 1: Построить матрицу всех циклических сдвигов**

Создаём все возможные циклические перестановки строки:

```
Сдвиг 0: B A N A N A $
Сдвиг 1: A N A N A $ B
Сдвиг 2: N A N A $ B A
Сдвиг 3: A N A $ B A N
Сдвиг 4: N A $ B A N A
Сдвиг 5: A $ B A N A N
Сдвиг 6: $ B A N A N A
```

**Шаг 2: Отсортировать строки лексикографически**

```
         1  2  3  4  5  6  7
     +--+--+--+--+--+--+--+
  0: | $| B| A| N| A| N| A|   (бывший сдвиг 6)
  1: | A| $| B| A| N| A| N|   (бывший сдвиг 5)
  2: | A| N| A| $| B| A| N|   (бывший сдвиг 3)
  3: | A| N| A| N| A| $| B|   (бывший сдвиг 1)
  4: | B| A| N| A| N| A| $|   (бывший сдвиг 0)  <-- исходная строка
  5: | N| A| $| B| A| N| A|   (бывший сдвиг 4)
  6: | N| A| N| A| $| B| A|   (бывший сдвиг 2)
     +--+--+--+--+--+--+--+
      ^                    ^
      |                    |
      первый столбец       последний столбец (результат BWT)
```

**Шаг 3: Взять последний столбец**

```
Последний столбец (BWT): A N N B $ A A

Индекс исходной строки: 4 (строка 4 в отсортированной матрице -- это исходная строка)
```

**Результат BWT:** строка `ANNB$AA` и индекс `4`.

#### ASCII-art матрицы

```
Отсортированная матрица:        Первый столбец | Последний столбец
                                       |                |
                                       v                v
    $ B A N A N [A]  <-- строка 0      $               A
    A $ B A N A [N]  <-- строка 1      A               N
    A N A $ B A [N]  <-- строка 2      A               N
    A N A N A $ [B]  <-- строка 3      A               B
  * B A N A N A [$]  <-- строка 4      B               $   (* = исходная)
    N A $ B A N [A]  <-- строка 5      N               A
    N A N A $ B [A]  <-- строка 6      N               A
                 ^
                 |
           BWT результат: A N N B $ A A
```

#### Почему BWT группирует одинаковые символы

Ключевое наблюдение: **последний столбец содержит символы, которые предшествуют символам первого столбца** в исходной строке.

Первый столбец (отсортированный): `$ A A A B N N`

Все строки, начинающиеся с `A`, сгруппированы вместе (строки 1-3). Последний столбец для этих строк содержит символы, стоящие **перед** `A` в исходной строке. Если в тексте часто встречается паттерн `NA` (как в BA**NA**NA), то перед несколькими `A` будет стоять `N`, и эти `N` окажутся рядом в последнем столбце.

В естественных текстах это свойство работает блестяще: перед буквой `e` в английском чаще всего стоят `h`, `r`, `s` -- и эти буквы группируются в BWT-выходе.

### 7.3 Move-to-Front (MTF) преобразование

После BWT одинаковые символы стоят рядом. MTF преобразует эту строку в последовательность малых чисел (много нулей и единиц), которые отлично сжимаются кодированием Хаффмана.

**Алгоритм:**
1. Поддерживаем список всех символов алфавита (изначально в порядке ASCII).
2. Для каждого символа входа:
   a. Записываем его индекс в текущем списке.
   b. Перемещаем этот символ в начало списка.

#### Пошаговый пример

Вход (результат BWT): `A N N B $ A A`

Начальный список: `[$ A B N]` (отсортированный алфавит нашей строки)

```
Шаг 1: символ = A
  Индекс A в списке: 1
  Вывод: 1
  Перемещаем A в начало: [A $ B N]

Шаг 2: символ = N
  Индекс N в списке: 3
  Вывод: 3
  Перемещаем N в начало: [N A $ B]

Шаг 3: символ = N
  Индекс N в списке: 0  (N уже в начале!)
  Вывод: 0
  Список не меняется: [N A $ B]

Шаг 4: символ = B
  Индекс B в списке: 3
  Вывод: 3
  Перемещаем B в начало: [B N A $]

Шаг 5: символ = $
  Индекс $ в списке: 3
  Вывод: 3
  Перемещаем $ в начало: [$ B N A]

Шаг 6: символ = A
  Индекс A в списке: 3
  Вывод: 3
  Перемещаем A в начало: [A $ B N]

Шаг 7: символ = A
  Индекс A в списке: 0  (A уже в начале!)
  Вывод: 0
  Список не меняется: [A $ B N]
```

**Результат MTF:** `[1, 3, 0, 3, 3, 3, 0]`

Обратите внимание: появились нули (повторяющиеся символы). На реальных данных после BWT последовательность MTF содержит **очень много нулей и малых чисел**, что делает её идеальной для энтропийного кодирования.

### 7.4 Затем -- кодирование Хаффмана

Последовательность MTF `[1, 3, 0, 3, 3, 3, 0]` содержит:
- 0: 2 раза (28.6%)
- 3: 4 раза (57.1%)
- 1: 1 раз (14.3%)

Хаффман присвоит:
- 3 -> `0` (1 бит, самый частый)
- 0 -> `10` (2 бита)
- 1 -> `11` (2 бита)

Закодированная последовательность: `11 0 10 0 0 0 10` = 10 бит.

Исходная строка `BANANA$` = 7 символов * 8 бит = 56 бит.
Сжатый вариант: ~10 бит + служебная информация.

### 7.5 Почему BZip2 медленнее, но сжимает лучше

| Свойство                 | BZip2                           | GZIP                          |
|-------------------------|----------------------------------|-------------------------------|
| Основной алгоритм        | BWT + MTF + Хаффман             | LZ77 + Хаффман                |
| Размер блока             | 100-900 КБ                     | 32 КБ (окно)                 |
| Сложность BWT            | O(n * log(n)) на сортировку     | --                            |
| Скорость сжатия           | ~10-30 МБ/с                    | ~50-100 МБ/с                  |
| Скорость декомпрессии     | ~30-50 МБ/с                    | ~300-400 МБ/с                 |
| Степень сжатия            | ~4-5x                          | ~3-4x                         |

BWT требует **сортировки** всех циклических сдвигов блока -- это затратная операция. Но BWT группирует данные настолько эффективно, что итоговое сжатие оказывается лучше.

### 7.6 Использование

- **Архивация файлов** -- `.bz2`-файлы, дистрибутивы исходных кодов Linux
- **Hadoop** -- поддерживается как кодек (но медленный)
- **НЕ применяется** в стриминге и real-time обработке из-за очень медленной декомпрессии
- В настоящее время BZip2 вытеснен ZSTD на высоких уровнях сжатия (ZSTD сжимает так же хорошо, но в 5-10 раз быстрее)

---

## 8. Run-Length Encoding (RLE)

### 8.1 Простейшее сжатие

RLE (кодирование длин серий) -- простейший алгоритм сжатия. Его идея: заменить **последовательность повторяющихся значений** парой **(значение, количество повторений)**.

### 8.2 Примеры

#### Хороший случай: много повторений

```
Исходные данные:  A A A B B B C C
RLE-кодирование:  (A, 3) (B, 3) (C, 2)

Или в компактной записи:
  Вход:   AAABBBCC
  Выход:  3A 3B 2C

  Вход:   8 байтов
  Выход:  6 байтов (6 символов в записи, или 3 пары по 2 байта)
  Сжатие: 1.33x
```

#### Столбец целых чисел (колоночные данные)

```
Исходный столбец: [5, 5, 5, 5, 3, 3, 7]

RLE-кодирование:  [(значение=5, кол-во=4),
                   (значение=3, кол-во=2),
                   (значение=7, кол-во=1)]

  Вход:   7 значений * 4 байта = 28 байтов
  Выход:  3 пары * (4+4) байта = 24 байта
  Сжатие: 1.17x
```

#### Плохой случай: нет повторений

```
Вход:   A B C D E F
RLE:    (A,1) (B,1) (C,1) (D,1) (E,1) (F,1)

В компактной записи: 1A 1B 1C 1D 1E 1F

  Вход:   6 байтов
  Выход:  12 байтов (стало в 2 раза БОЛЬШЕ!)
```

Поэтому RLE эффективен только для данных с длинными сериями одинаковых значений.

### 8.3 Bit-packing: упаковка малых целых в биты

Bit-packing -- это не сжатие в традиционном смысле, а компактное представление целых чисел, которым не нужны все 32 (или 64) бита.

```
Пример: массив значений [0, 1, 2, 3, 1, 0, 2, 3]
  Максимальное значение: 3
  Количество бит для представления: ceil(log2(3+1)) = 2 бита

  Стандартное хранение (32-bit int):
    00000000 00000000 00000000 00000000  = 0
    00000000 00000000 00000000 00000001  = 1
    00000000 00000000 00000000 00000010  = 2
    ...
    8 значений * 4 байта = 32 байта

  Bit-packed (2 бита на значение):
    00 01 10 11 01 00 10 11
    = 0x6D 0x4B
    8 значений * 2 бита = 16 бит = 2 байта

  Сжатие: 16x
```

### 8.4 Гибрид RLE + Bit-packing в Parquet

Apache Parquet использует комбинированную схему **RLE/Bit-Packing Hybrid Encoding** для кодирования уровней определения (definition levels) и повторения (repetition levels), а также словарных индексов.

Формат:

```
Каждая группа начинается с заголовка (varint):
  - Если младший бит = 0: RLE-серия
    Заголовок >> 1 = количество повторений
    За заголовком: значение, занимающее ceil(bit_width / 8) байт

  - Если младший бит = 1: Bit-packed серия
    (Заголовок >> 1) * 8 = количество значений
    За заголовком: значения, упакованные по bit_width бит каждое

Пример (bit_width=3): [5,5,5,5,5,5,5,5, 2,3,1,0,7,4,6,5]
  [16][05]         -- RLE: заголовок=(8<<1)|0=16, значение=5 (8 пятёрок)
  [03][D2 6B E5]   -- bit-pack: заголовок=(1<<1)|1=3, 8 значений по 3 бита
```

### 8.5 Когда RLE эффективно

- **Отсортированные данные** -- после сортировки одинаковые значения гарантированно стоят рядом.
- **Низкая кардинальность** -- столбец `статус_заказа` с 5 возможными значениями.
- **Словарные индексы** -- после dictionary encoding столбец с миллионами строк превращается в массив малых индексов, часто с длинными сериями.
- **Bitmap-индексы** -- массивы из 0 и 1 идеально подходят для RLE.
- **Разреженные данные** -- столбец, где 95% значений = NULL (или 0).

---

## 9. Словарное кодирование (Dictionary Encoding)

### 9.1 Что это такое

Словарное кодирование (Dictionary Encoding) -- это **не кодек сжатия** в том же смысле, что GZIP или LZ4. Это **схема кодирования**, используемая в колоночных форматах (Parquet, ORC) для эффективного представления повторяющихся значений.

### 9.2 Как работает

**Шаг 1:** Проходим по всем значениям столбца и строим **словарь** (dictionary) -- упорядоченный список уникальных значений.

**Шаг 2:** Заменяем каждое значение в столбце его **индексом** в словаре.

**Шаг 3:** Храним словарь + массив индексов вместо исходных данных.

### 9.3 Конкретный пример

Столбец `country` в таблице с 10 000 000 строк:

```
Исходные данные (10M строк):
  ["Россия", "Россия", "Китай", "США", "Россия", "Германия",
   "Китай", "Россия", "Россия", "Китай", "США", "Россия", ...]

Уникальные значения (10 стран):
  Россия, Китай, США, Германия, Франция, Бразилия,
  Индия, Япония, Канада, Австралия
```

**Без словарного кодирования:**

```
10 000 000 строк * среднее ~10 байт на строку (UTF-8) = ~100 МБ
```

**Со словарным кодированием:**

```
Словарь (10 записей):
  0: "Австралия"
  1: "Бразилия"
  2: "Германия"
  3: "Индия"
  4: "Канада"
  5: "Китай"
  6: "Россия"
  7: "США"
  8: "Франция"
  9: "Япония"

Размер словаря: 10 * ~10 байт = ~100 байтов

Массив индексов:
  [6, 6, 5, 7, 6, 2, 5, 6, 6, 5, 7, 6, ...]

Каждый индекс: 0-9, нужно ceil(log2(10)) = 4 бита
  10 000 000 * 4 бита = 5 000 000 байт = ~5 МБ

Итого: ~100 байт (словарь) + ~5 МБ (индексы) = ~5 МБ
Сжатие: 100 МБ -> 5 МБ = 20x
```

### 9.4 Комбинация с RLE или Bit-packing

Массив индексов сам по себе хорошо поддаётся дальнейшему сжатию:

```
Массив индексов (отсортированная таблица):
  [0, 0, 0, ..., 1, 1, 1, ..., 2, 2, ..., 9, 9, 9]
   ^^^^^^^       ^^^^^^^       ^^^^       ^^^^^^^
   1M нулей      1M единиц    500K двоек  800K девяток

RLE: [(0, 1000000), (1, 1000000), (2, 500000), ..., (9, 800000)]
     10 пар * 8 байтов = 80 байтов!

Итого с RLE: 100 байт (словарь) + 80 байт (RLE-индексы) = 180 байт
             для 10M строк! Сжатие: ~555 000x
```

На практике данные редко бывают полностью отсортированы, но даже частичная сортировка даёт значительный выигрыш.

Для неотсортированных данных используется **bit-packing** (4 бита на индекс): 10M значений * 4 бита = 5 МБ.

### 9.5 Ограничение размера словаря и fallback в Parquet

В Apache Parquet словарное кодирование применяется автоматически, но имеет ограничение:

Правило Parquet: если размер словаря превышает размер страницы данных (по умолчанию 1 МБ), словарное кодирование отключается, и Parquet переключается на PLAIN-кодирование. Это происходит постранично.

Пример: столбец `email` (уникальный для каждого пользователя). После 50 000 записей словарь занимает > 1 МБ. Parquet: страница 1 -- DICT, страница 2+ -- PLAIN. В метаданных: `encodings = [DICT, PLAIN]`.

### 9.6 Когда словарное кодирование работает хорошо

| Сценарий                                   | Кардинальность    | Эффективность |
|--------------------------------------------|-------------------|---------------|
| Столбец `страна` (10 значений)             | Очень низкая      | Отличная (20x+) |
| Столбец `статус_заказа` (5 значений)       | Очень низкая      | Отличная (50x+) |
| Столбец `город` (10 000 значений)          | Средняя           | Хорошая (5-10x) |
| Столбец `имя_пользователя` (1M значений)   | Высокая           | Плохая (словарь слишком большой) |
| Столбец `email` (уникальные)               | Равна числу строк  | Бесполезна (fallback на PLAIN) |
| Столбец `UUID` (уникальные)                | Равна числу строк  | Бесполезна |

**Правило большого пальца:** словарное кодирование эффективно, когда кардинальность столбца << количества строк. Если кардинальность > 10 000, стоит проверить, не превышает ли словарь лимит.

---

## 10. Практическое сравнение

### 10.1 Сводная таблица

Приблизительные показатели на типичных данных (текстовые/табличные данные, один поток, современный CPU):

| Кодек       | Степень сжатия | Скорость сжатия (МБ/с) | Скорость декомпрессии (МБ/с) | Нагрузка на CPU | Потребление RAM  | Лучшее применение                      |
|------------|----------------|------------------------|------------------------------|-----------------|------------------|-----------------------------------------|
| **LZ4**    | 2.0-2.5x       | 700-800                | 4000-5000                    | Очень низкая    | Минимальное (~64 КБ) | Real-time, стриминг, кэширование     |
| **Snappy** | 1.5-2.5x       | 500-600                | 1500-2000                    | Низкая          | Минимальное (~64 КБ) | Real-time, по умолчанию в Parquet    |
| **ZSTD-1** | 2.5-2.8x       | 400-500                | 1200-1500                    | Низкая          | ~1 МБ            | Замена Snappy (лучше по всем метрикам) |
| **ZSTD-3** | 2.8-3.2x       | 200-300                | 1000-1200                    | Средняя         | ~2 МБ            | Общего назначения                      |
| **GZIP-1** | 2.5-3.0x       | 80-120                 | 300-400                      | Средняя         | ~256 КБ          | Совместимость, HTTP                    |
| **GZIP-6** | 3.0-3.5x       | 30-60                  | 300-400                      | Высокая         | ~256 КБ          | Архивные данные                        |
| **GZIP-9** | 3.2-3.8x       | 10-20                  | 300-400                      | Высокая         | ~256 КБ          | Максимальное сжатие GZIP               |
| **ZSTD-9** | 3.3-3.7x       | 50-80                  | 800-1000                     | Средняя         | ~8 МБ            | Баланс размера и скорости              |
| **ZSTD-19**| 3.6-4.0x       | 5-10                   | 700-900                      | Очень высокая   | ~64 МБ           | Холодные данные, максимальное сжатие   |
| **BZip2**  | 3.5-5.0x       | 10-30                  | 30-50                        | Высокая         | ~8 МБ            | Архивация (устарел, замена на ZSTD)   |

> **Примечание:** числа приблизительные и сильно зависят от типа данных. На высокоповторяющихся данных степень сжатия может быть значительно выше, на случайных (энтропийных) данных -- ниже.

### 10.2 Блок-схема принятия решения

```
                   Какой кодек выбрать?
                          |
                          v
              Нужна максимальная скорость
               декомпрессии (real-time)?
                     /          \
                   Да            Нет
                   |              |
                   v              v
                 LZ4          Нужна максимальная
                              степень сжатия?
                              /          \
                            Да            Нет
                            |              |
                            v              v
                      ZSTD level       Нужна быстрая
                        16-22          запись (ingestion)?
                                       /          \
                                     Да            Нет
                                     |              |
                                     v              v
                                ZSTD level     ZSTD level
                                   1-3            6-9
                                     |
                          (замена Snappy/LZ4     (баланс
                           с лучшим сжатием)      скорости
                                                  и сжатия)

              Нужна совместимость со старыми системами?
                     /          \
                   Да            Нет
                   |              |
                   v              v
                 GZIP          ZSTD (лучший
                (универсальный  выбор по умолчанию)
                 стандарт)
```

### 10.3 Какой кодек с каким форматом

| Формат  | Кодек по умолчанию | Рекомендуемый кодек   | Обоснование                                 |
|---------|--------------------|-----------------------|---------------------------------------------|
| **Parquet** | Snappy         | **ZSTD (level 3)**    | ZSTD сжимает на 30-50% лучше при сопоставимой скорости чтения |
| **Avro**    | None (без сжатия) | **Snappy** или **ZSTD** | Snappy для низкой задержки, ZSTD для экономии места |
| **ORC**     | ZLIB           | **ZSTD**              | ZSTD быстрее ZLIB при аналогичном сжатии    |
| **Feather** | LZ4            | **LZ4** или **ZSTD**  | LZ4 для интерактивного анализа, ZSTD для хранения |

#### Примеры настройки

```python
# PySpark: Parquet с ZSTD
df.write.option("compression", "zstd").parquet("output/")
# PySpark: настройка уровня ZSTD
spark.conf.set("spark.sql.parquet.compression.codec", "zstd")
spark.conf.set("io.compression.codec.zstd.level", "3")

# PyArrow: Parquet с ZSTD level 3
import pyarrow.parquet as pq
pq.write_table(table, "output.parquet", compression="zstd", compression_level=3)
# PyArrow: Parquet со Snappy
pq.write_table(table, "output.parquet", compression="snappy")
```

---

## 11. Матрица поддержки кодеков

### 11.1 Какие кодеки поддерживают какие форматы

| Кодек         | Parquet | Avro   | ORC    | Feather / Arrow IPC |
|---------------|---------|--------|--------|---------------------|
| **None**      | Да      | Да     | Да     | Да                  |
| **Snappy**    | Да (по умолчанию) | Да | Да | Да                |
| **GZIP**      | Да      | Да     | Нет    | Нет                 |
| **ZLIB**      | Нет     | Нет    | Да (по умолчанию) | Нет      |
| **LZ4**       | Да      | Нет    | Да     | Да (по умолчанию)   |
| **LZ4_RAW**   | Да      | Нет    | Нет    | Нет                 |
| **ZSTD**      | Да      | Да     | Да     | Да                  |
| **BZip2**     | Нет     | Да     | Нет    | Нет                 |
| **Deflate**   | Нет     | Да     | Нет    | Нет                 |
| **XZ/LZMA**   | Нет     | Да     | Нет    | Нет                 |
| **LZO**       | Да      | Нет    | Да     | Нет                 |
| **Brotli**    | Да      | Нет    | Нет    | Нет                 |

### 11.2 Рекомендации по выбору

| Если вы используете...           | Формат  | Кодек                               |
|----------------------------------|---------|--------------------------------------|
| Spark/Hive (аналитический склад) | Parquet | ZSTD level 3 (или Snappy для совместимости) |
| Kafka / стриминг                 | Avro    | Snappy (минимальная задержка)        |
| Hive (legacy)                    | ORC     | ZSTD (вместо ZLIB)                   |
| Pandas / R (интерактивный анализ)| Feather | LZ4 (скорость) или ZSTD (экономия)  |
| Архив / холодное хранилище       | Parquet | ZSTD level 9-19 или GZIP-9          |
| ML / обучение моделей            | Parquet | Snappy / ZSTD-1 (скорость чтения)   |

---

## Заключение

Сжатие данных -- одна из самых важных оптимизаций в системах обработки больших данных. Ключевые выводы:

1. **I/O -- узкое место.** Сжатие ускоряет обработку, потому что меньше данных нужно читать с диска и передавать по сети. Время декомпрессии почти всегда компенсируется экономией на I/O.

2. **Все современные кодеки основаны на LZ77.** Snappy, LZ4, GZIP, ZSTD -- все используют принцип скользящего окна и обратных ссылок. Разница в том, что добавляется поверх: ничего (Snappy, LZ4), Хаффман (GZIP), Хаффман + FSE (ZSTD).

3. **ZSTD -- лучший универсальный выбор.** На каждом уровне скорости ZSTD сжимает лучше альтернатив. Рекомендуется использовать ZSTD level 1-3 вместо Snappy и ZSTD level 6-9 вместо GZIP.

4. **Словарное кодирование и RLE -- не замена, а дополнение.** Колоночные форматы (Parquet, ORC) применяют словарное кодирование и RLE **до** кодеков сжатия. Это двухуровневая система: сначала кодирование (dictionary + RLE/bit-packing), затем сжатие (Snappy/ZSTD/GZIP).

5. **Выбор кодека зависит от сценария.** Real-time -> LZ4/Snappy. Общего назначения -> ZSTD-3. Архив -> ZSTD-19. Совместимость -> GZIP.

```
Иерархия сжатия в колоночных форматах:

  Исходный столбец
       |
       v
  [Dictionary Encoding]     -- уровень 1: устраняет дубликаты строк
       |
       v
  [RLE / Bit-packing]       -- уровень 2: сжимает индексы / уровни
       |
       v
  [Кодек сжатия]            -- уровень 3: блочное сжатие (Snappy/ZSTD/GZIP)
       |
       v
  Сжатые данные на диске
```

---

> **Дополнительные материалы:**
> - RFC 1951 (DEFLATE), RFC 1952 (GZIP)
> - Оригинальная статья LZ77: Ziv, Lempel. "A Universal Algorithm for Sequential Data Compression" (1977)
> - Спецификация ZSTD: RFC 8878
> - Спецификация Parquet: https://parquet.apache.org/documentation/latest/
> - Исходный код ZSTD: https://github.com/facebook/zstd
> - Исходный код LZ4: https://github.com/lz4/lz4
> - Benchmarks: https://github.com/facebook/zstd#benchmarks
