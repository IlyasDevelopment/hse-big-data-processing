# Семинар 2 — Сжатие и форматы файлов для больших данных / ETL

## Структура семинара (~80 мин)

| # | Часть | Продолжительность | Материал |
|---|-------|-------------------|----------|
| 1 | Глубокий разбор формата Parquet | ~30 мин | `parquet_format_presentation.pptx` |
| 2 | Теория: Avro, ORC, CSV, JSON, Arrow | ~20 мин | `columnar_and_row_formats_theory.md` |
| 3 | Практический бенчмарк | ~20-30 мин | Python-скрипты (см. ниже) |

## Файлы

### Презентация и теория

- **`parquet_format_presentation.pptx`** — Слайды, охватывающие внутреннее устройство Parquet: группы строк, фрагменты столбцов, страницы, кодирование (RLE_DICTIONARY, PLAIN), кодеки сжатия, предикатный pushdown, партиционирование, Delta Lake.
- **`columnar_and_row_formats_theory.md`** — Конспект лекции по форматам Avro, ORC, CSV, JSON, Arrow/Feather. Включает сравнительные таблицы, обзор алгоритмов сжатия (Snappy, GZIP, LZ4, ZSTD) и рекомендации по выбору формата.

### Практический бенчмарк — всё в одном файле

- **`benchmark_formats.py`** — Бенчмарк в одном файле, который генерирует ~3 ГБ синтетических данных (30 млн строк) и сравнивает 15 вариантов формат/сжатие (CSV, JSON Lines, Parquet, Avro, ORC, Feather). Измеряет время записи, размер файла, полное чтение, выбор столбцов, чтение с фильтрацией и агрегацию. Использует потоковую запись для экономии памяти.
  ```bash
  pip install -r requirements.txt
  python benchmark_formats.py          # полный запуск (~3 ГБ CSV, 15-30 мин)
  python benchmark_formats.py --quick  # сокращённый запуск (~500 МБ, 5-7 мин)
  ```

### Практический бенчмарк — пошагово (для разбора в аудитории)

Запускайте скрипты по порядку. Каждый шаг независим и сохраняет результаты для следующего.

- **`common.py`** — Общие импорты, константы и вспомогательные функции, используемые всеми шагами.
- **`step_1_generate_data.py`** — Генерирует 30 млн синтетических строк событий и записывает их в Parquet-файл потоково. Никогда не держит весь набор данных в памяти.
  ```bash
  python step_1_generate_data.py                     # 30 млн строк (по умолчанию)
  python step_1_generate_data.py --num-rows 5000000  # 5 млн строк (быстрый режим)
  ```
- **`step_2_write_formats.py`** — Считывает сгенерированные данные Parquet и записывает их во все 15 вариантов форматов. Измеряет время записи и размер файла.
  ```bash
  python step_2_write_formats.py
  ```
- **`step_3_read_benchmarks.py`** — Считывает каждый записанный файл и измеряет: полное сканирование, выбор столбцов, чтение с фильтрацией и агрегацию с группировкой.
  ```bash
  python step_3_read_benchmarks.py
  ```
- **`step_4_visualize_results.py`** — Объединяет результаты записи и чтения, выводит сводную таблицу, генерирует сравнительные столбчатые диаграммы (PNG) и выделяет ключевые выводы.
  ```bash
  python step_4_visualize_results.py
  ```

### Прочее

- **`requirements.txt`** — Зависимости Python.

## Требования

```
Python 3.10+
pip install -r requirements.txt
```
